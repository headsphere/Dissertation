---
header-includes:
   - \usepackage[]{algpseudocode}
---


\chapter{HMM Model Derivation}

# Independent Mixture Model

This section will outline the derivation of the HMM for univariate Poisson observed data. We will use a toy example in order to illustrate the derivation.

An independent mixture model consists of $m$ component distributions with probability functions $p_i$ for $i \in {1, ... , m}$ and a “mixing distribution”. The mixing is performed by a discrete random variable $C$:
$$C =
  \begin{cases}
   1        & \text{with probability } \delta_1 \\
   \vdots   & \vdots \\
   i        & \text{with probability } \delta_i \\
   \vdots   & \vdots \\
   m        & \text{with probability } \delta_m = 1 - \sum_{i=1}^{m-1} \delta_i
  \end{cases}$$

Thus $Pr(C=i)=\delta_i$ must obey $0 < \delta_i < 1$ and that  $\sum_{i=1}^m \delta_i = 1$

For a discrete random variable X described by a mixture model consisting of $m$ components, it holds that:

$$p(X) = \sum_{i=1}^m \delta_i p_i(X) \implies PR(X = x) = \sum_{i=1}^m Pr(X = x|C = i)$$

To estimate the parameters $\theta$ of the mixture distribution via ML estimation we maximize the combined likelihood of the components:

$$L(\theta_1,...,\theta_m,\delta_1,...,\delta_m|x_1,...,x_n) = \prod_{j=1}^n \sum_{i=1}^m \delta_i p_i(x_j, \theta_i)$$

For the example where $m = 2$ and the component distributions are Poisson with parameters $\lambda_1$ and $\lambda_2$ the likelihood equation is as follows:

$$L(\lambda_1,\lambda_2,\delta_1,\delta_2 | x_1,...,x_n) = \prod_{j=1}^n \Bigg( \delta_1 \frac{\lambda_1^{x_j} e^{-\lambda_1}}{x_j !} + \delta_2 \frac{\lambda_2^{x_j} e^{-\lambda_2}}{x_j !} \Bigg)$$

In R this could be implemented naively as follows:

```{r}
lambdas = c(3, 15)
deltas = c(0.3, 0.7)
cdf = cumsum(deltas)
unifvec = runif(100)
d1 = rpois(sum(unifvec < cdf[1]),lambdas[1])
d2 = rpois(sum(unifvec >= cdf[1]),lambdas[2])
x = c(d1,d2) 
```

This code generates a series of x values according to a mixture of two Poisson distributions with $\lambda_1=`r lambdas[1]`, \lambda_2=`r lambdas[2]`, \delta_1=`r deltas[1]`$ and $\delta_2=`r deltas[2]`$

We can see from both the following plots of the time series and the kernel density estimates, the data appears to conform to two separate distributions, or states, as it were:  

```{r, fig.width = 10, fig.height = 2, fig.fullwidth = TRUE, echo=FALSE,fig.cap="Time series of simulated Poisson mixture"}
library(ggplot2)
library(reshape2)
df = data.frame(Time = 1:100, Values = x)
df_melt = melt(df, id.vars = 'Time')
ggplot(df_melt, aes(x = Time, y = value)) + 
    geom_line()
```

```{r, fig.width = 10, fig.height = 2, fig.fullwidth = TRUE, echo=FALSE, fig.cap="Kernel density estimate of simulated Poisson mixture (bandwidth=30, kernel=gaussian)"}
ggplot(df_melt,aes(x=x, fill=variable)) + geom_density(alpha=0.25)
```

The kernel density estimates are generated with the Gaussian kernel and a bandwidth of `r round(bw.nrd0(x),2)`. The multimodal appearance of the plot clearly shows that the data are overdispersed compared to a single Poisson, and instead appear to come from a mixture of two Poissons. 

Before minimising the negative log-likelihood, the final step is that we must apply a transformation of the parameters in order to fulfill the requirements that $\sum_i \delta_i = 1$, $\delta_i > 0$ and $\lambda_i > 0$ for all $i$. This is achieved by log-transforming $\lambda$ and logit transforming $\delta$.

\begin{align}
 \begin{aligned}\label{eq:paramN2W}
    \text{Log-transform } \quad   & \eta_i = log \lambda_i, i = 1,...,m \\
    \text{Logit-transform } \quad & \tau_i = log \Bigg( \frac{\delta_1}{1 - \delta_2}\Bigg)
  \end{aligned}
\end{align}

In R:
```{r eval=TRUE}
logit <- function(delta2) log(delta2/(1-delta2))

eta = log(lambdas)
tau = logit(deltas[2])
```

After the maximisation has occured the original parameters can be recovered by the inverse transformations:

\begin{align}
 \begin{aligned}\label{eq:paramW2N}
  \text{exp-transform }\quad &  \lambda_i = e^{\eta_i}, i = 1,...,m \\
  \text{inverse logit }\quad &  \delta_2 = \frac{e^{\tau}}{1 + e^{\tau}}
  \end{aligned}
\end{align}

and additionally $\delta_1 = 1 - \delta_2$

```{r eval=TRUE}
invlogit <- function(tau) exp(tau)/(1+exp(tau))
lambdas = exp(eta)
delta2 = invlogit(tau)
delta1 = 1 - delta2
```

Now we have all the building blocks to maximise the negative log-likelihood for a two-state mixture distribution

```{r warning=FALSE, cache=FALSE}
f <- function(theta,data) {
  eta <- theta[1:2]
  tau <- theta[3]
  lambdas = exp(eta) 
  delta2 = invlogit(tau)
  delta1 = 1-delta2

  L = delta1*dpois(data,lambdas[1]) + delta2*dpois(data,lambdas[2])
  -sum(log(L))
}
```

Use the 25% and 75% quantiles of the data as a guess for lambdas:
```{r}
quantiles <- quantile(x)
lambdaGuess = c(quantiles[2],quantiles[4]) 
```

We can now optimise using the R `nlm` package:
```{r warning=FALSE}
deltaGuess = 0.1
theta <- c(log(lambdaGuess), logit(deltaGuess))
res = nlm(f, theta, data = x)
```

And back transform the results:
```{r}
lambda1Hat = exp(res$estimate[1])
lambda2Hat = exp(res$estimate[2])
delta2Hat = invlogit(res$estimate[3])
delta1Hat = 1-delta2Hat
```

Which provides us with the following estimated values:

$$\boldsymbol{\lambda} = 
\begin{pmatrix}
 `r lambdas[1]` \\
 `r lambdas[2]`
\end{pmatrix} \qquad
\boldsymbol{\hat{\lambda}} = 
\begin{pmatrix}
 `r lambda1Hat` \\
 `r lambda2Hat`
\end{pmatrix}$$

$$\boldsymbol{\delta} = 
\begin{pmatrix}
 `r deltas[1]` \\
 `r deltas[2]`
\end{pmatrix} \qquad
\boldsymbol{\hat{\delta}} = 
\begin{pmatrix}
 `r delta1Hat` \\
 `r delta2Hat`
\end{pmatrix}$$

The mixture distribution parameters appear to be estimated satisfactorily when compared with the true values from the simulated data.

The next step is to introduce into the model a way of capturing the transition behaviour between the states over time.

# Markov switching process

The independent mixture model is used to cater for the overdispersion noticed in some time series data. The second feature often found is that they exhibit serial dependece between the observations. This dependence is identified via the autocorrelation function of the data. To cater for this we model the dependence between the mixtures by introducing a Markov process. With this building block we will have completely defined the hidden Markov model. 

If we define $\{C_t : t = 1,2, \dots T \}$ as the unobserved hidden states, and $\{X_t : t = 1,2, \dots T \}$ as the observed data points, and $X^{(t)}$ denotes the sequence of all R.V.s $X_i$ for $i = 1 \dots t$, we can say a hidden Markov is a dependent mixture where:

$$Pr(C_t = i | C^{(t-1)}) = Pr(C_t = i | C_{t-1}), t = 2,3,\dots$$
$$Pr(X_t = x | X^{(t-1)}, C^{(t)}) = Pr(X_t = x | C_t), t \in \mathbb{N}$$

As is illustrated in Figure \ref{fig:hmm-states}, this is saying that when $C_t$ is known the distribution of $X_t$ only depends on $C_t$

\begin{marginfigure}
  \includegraphics{images/hmm-states}
  \caption{Hidden state directed graph}
  \label{fig:hmm-states}
\end{marginfigure}

For discrete distributions we can define the m-state dependent distribution function as:

\begin{equation} \label{eq:dependantDist}
  p_i(X) = Pr(X_t = x | C_t = i), i,2,\dots,m
\end{equation}

Which is simply the conditional probability of observing $x$ at time $t$ when $C$ is in state $i$.

Additionally we can define the unconditional probability of being in state $i$ at time $t$ as:

\begin{equation} \label{eq:stateDist}
  u_i(t) = Pr(C_t = i), t = 1,\dots,T
\end{equation}

We will often want to calculate the marginal distribution of $X$. This can be achieved by summing over the states and using the hidden state $\eqref{eq:stateDist}$ and state-dependent $\eqref{eq:dependantDist}$ distributions:

\begin{align}
 \begin{aligned}\label{eq:marginalNonStatWorking}
    Pr(X_t = x) &= \sum_{i=1}^m Pr(C_t = i) Pr(X_t = x | C_t = i) \\
                &= \sum_{i=1}^m u_i(t) p_i(x) \\
                &= \begin{pmatrix}
                     u_1(t),\dots,u_m(t) 
                   \end{pmatrix}
                   \begin{pmatrix}
                     p_1(x) &        & 0 \\
                            & \ddots &   \\
                     0      &        & p_m(x) 
                   \end{pmatrix}
                   \begin{pmatrix}
                     1 \\
                     \vdots \\
                     1 
                   \end{pmatrix} \\
                &= \mathbf{u}(t) \mathbf{P}(x) \mathbf{1'}
 \end{aligned}
 \end{align}
 
$\mathbf{u}(t)$ is the distribution of the hidden states, and $\mathbf{P}(x)$ is the state-dependent distribution of the observations. $\mathbf{u}(1)$ is defined as the initial distribution of the Markov chain.

The $m \times m$ diagonal matrix of conditional probabilities $\mathbf{P}(x)$ defined as:
$$\mathbf{P}(x) = \begin{pmatrix}
                   p_1(x) &        & 0 \\
                          & \ddots &   \\
                   0      &        & p_m(x) 
                 \end{pmatrix}$$

Where for example: $\boldsymbol{\lambda} = \begin{pmatrix}
                           `r lambdas[1]` \\
                           `r lambdas[2]`
                         \end{pmatrix}$
$$\mathbf{P}(1) = \begin{pmatrix}
 \frac{e^{-`r lambdas[1]`} `r lambdas[1]`^1}{1!} & 0 \\
 0                     & \frac{e^{-`r lambdas[2]`} `r lambdas[2]`^1}{1!} 
\end{pmatrix} = 
\begin{pmatrix}
 `r dpois(1, lambdas[1])` & 0 \\
 0                        & `r dpois(1, lambdas[2])`
\end{pmatrix}$$

This can be implemented easily in R as:

```{r}
PMat <- function(x, lambda) diag(dpois(x, lambda))

PMat(1, lambdas)
```

Next we introduce the transition probability matrix $\boldsymbol{\Gamma}(t)$ which is the $m \times m$ matrix of transition probabilities at state $t$:

$$\boldsymbol{\Gamma}(1) = \begin{pmatrix}
                             \gamma_{11} & \cdots & \gamma_{1m} \\
                             \vdots      & \ddots & \vdots  \\
                             \gamma_{m1} & \cdots & \gamma_{mm}
                           \end{pmatrix}$$

$$\gamma_{ij}(t) = Pr(C_{t+1} = j | C_t = i)$$

Because the Markov chain satisfies the Chapman-Kolmogorov equations [@zucchini, p16] we can say that $\mathbf{u}(t) = \mathbf{u}(1) \boldsymbol{\Gamma}^{t-1}$ (where $\boldsymbol{\Gamma}$ is shorthand for $\boldsymbol{\Gamma}(1)$) and hence it follows that: 

\begin{equation} \label{eq:marginalNonStat}
  Pr(X_t = x) = \mathbf{u}(1) \boldsymbol{\Gamma}^{t-1} \mathbf{P}(x) \mathbf{1'}
\end{equation}

If the Markov chain can be shown to be stationary with stationary distribution $\boldsymbol{\delta}$, because $\boldsymbol{\delta} \boldsymbol{\Gamma}^{t-1} = \boldsymbol{\delta}$ for all $t$ we get:

\begin{equation} \label{eq:marginalStat}
  Pr(X_t = x) = \boldsymbol{\delta} \mathbf{P}(x) \mathbf{1'}
\end{equation}

It can be shown that if the stationary distribution exists then $\boldsymbol{\delta}$ can be found using the following calculation [proved in @zucchini, p19]:

\begin{equation} \label{eq:stationary}
  \boldsymbol{\delta} (\mathbf{I}_m - \boldsymbol{\Gamma} + \mathbf{U}) = \mathbf{1}
\end{equation}

Where $\mathbf{I}_m$ is an $m \times m$ identity matrix and $\mathbf{U}$ is an $m \times m$ matrix of ones. This can be implemented simply in R as follows:


```{r}
statdist <- function(gamma){
  m = dim(gamma)[1]
  matrix(1,1,m) %*% solve(diag(1,m) - gamma + matrix(1,m,m))
}
```

To illustrate this concept, we can extend the example we used previously this time introducing a hypothetical transition matrix:

```{r}
gamma = matrix(c(0.9,0.1,0.4,0.6), nrow = 2, byrow = TRUE)
```

$$\boldsymbol{\lambda} = \begin{pmatrix}
                           `r lambdas[1]` \\
                           `r lambdas[2]`
                         \end{pmatrix} \qquad
\boldsymbol{\Gamma} = \begin{pmatrix}
                        `r gamma[1,1]` & `r gamma[1,2]` \\
                        `r gamma[2,1]` & `r gamma[2,2]`
                      \end{pmatrix}$$

We can now calculate the stationary distribution $\boldsymbol{\delta}$ using equation $\eqref{eq:stationary}$:

```{r}
delta = statdist(gamma)
```

$$\boldsymbol{\delta} = \begin{pmatrix}
                           `r delta[1]` \\
                           `r delta[2]`
                         \end{pmatrix} \qquad$$
                         
With which we can now calculate the marginal distribution $\eqref{eq:marginalStat}$, for example for $P(X = 1)$:

```{r}
x = 1
sum(delta %*% PMat(x, lambdas))
```

# Calculating the Likelihood

The first step to calculate the likelihood function is to extend the univariate marginal distribution $\eqref{eq:marginalNonStat}$ to a bivariate distribution. @zucchini show that $\eqref{eq:marginalNonStat}$ can be extended so that:

$$Pr(X_t = v, X_{t+k} = w) = \mathbf{u}(t) \mathbf{P}(v) \boldsymbol{\Gamma}^k \mathbf{P}(w) \mathbf{1'}$$

While the equivalent for a stationary chain reduces to:

$$Pr(X_t = v, X_{t+k} = w) = \boldsymbol{\delta} \mathbf{P}(v) \boldsymbol{\Gamma}^k \mathbf{P}(w) \mathbf{1'}$$

This then can be generalised to a $T$-th order distribution and hence the likelihood for a set of observations $\mathbf{X}^{(T)} = (X_1,X_2,\dots,X_T)$ given the parameters of an HMM is:

\begin{equation}\label{eq:likelihood}
  L_T = \boldsymbol{\delta} \mathbf{P}(x_1) \boldsymbol{\Gamma} \mathbf{P}(x_2) \dots   \boldsymbol{\Gamma} \mathbf{P}(x_T) \mathbf{1'}
\end{equation}

A recursive algorithm can be defined to calculate the likelihood, first by defining the forward probability vector:

\begin{equation}\label{eq:forwardProb}
  \boldsymbol{\alpha_t} = \boldsymbol{\delta} \mathbf{P}(x_1) \boldsymbol{\Gamma} \mathbf{P}(x_2) \dots \boldsymbol{\Gamma} \mathbf{P}(x_T)
\end{equation}

From which the likelihood can now be recursively calculated:

\begin{align}
 \begin{aligned}\nonumber  
     \boldsymbol{\alpha_1} &= \boldsymbol{\delta} \mathbf{P}(x_1) \\
     \boldsymbol{\alpha_t} &= \boldsymbol{\alpha_{t-1}} \boldsymbol{\Gamma} \mathbf{P}(x_t) \qquad for \quad t = 2,3,\dots,T\\
     L_T                   &= \boldsymbol{\alpha_T} \mathbf{1'}
 \end{aligned}
 \end{align}

If $\boldsymbol{\delta}$ provided is in fact the stationary distribution of the Markov chain, then the recursive scheme becomes:

\begin{align}
 \begin{aligned}\nonumber  
     \boldsymbol{\alpha_0} &= \boldsymbol{\delta} \\
     \boldsymbol{\alpha_t} &= \boldsymbol{\alpha_{t-1}} \boldsymbol{\Gamma} \mathbf{P}(x_t) \qquad for \quad t = 1,2,\dots,T\\
     L_T                   &= \boldsymbol{\alpha_T} \mathbf{1'}
 \end{aligned}
 \end{align}
 
 With R implementation:

```{r}
pois.hmm.alpha <- function(x, gamma, delta = NULL)
{
  n = length(x)
  alpha = vector(mode="list", length=n)
  alpha0= statdist(gamma)
  
  for(t in 1:n)
  {
    if(t==1)
    {
      prevAlpha = alpha0
    }
    else
    {
      prevAlpha = alpha[[t-1]]
    }
    alpha[[t]] = prevAlpha %*% gamma %*% PMat(x[t],lambdas)
  }
  return(alpha[[n]])#return the last iteration
}
```

The problem with this scheme however is that as $t$ increases, $\boldsymbol{\alpha_t}$ rapidly diminishes as it is the product of a large number of probabilities, and when implemented computationally, will be rounded to zero (a problem known as underflow). We can show that the calculation works for small $t$ however needs to be re-worked for any non-trivial time series.

As an example we can calculate $Pr(X_1 = 0, X_2 = 2, X_3 = 1)$, using the previously defined $\boldsymbol{\Gamma}$ and $\boldsymbol{\lambda}$ parameters: 
```{r}
x = c(0,2,1)
alpha = pois.hmm.alpha(x, gamma)
prob = sum(alpha)
prob
```
@durbin [pp.99] present a method of scaling the computations to mitigate this issue of underflow. In summary they propose the following algorithm (where $\boldsymbol{\Gamma}$ and $\mathbf{P}(x_t)$ are $m \times m$ matrices, $\mathbf{v}$ and $\boldsymbol{\phi_t}$ are vectors of length $m$, $u$ is a scalar, and $l$ is the scalar which accumulates the log-likelihood).

\begin{algorithmic}
\State set $\phi_0 \gets \boldsymbol{\delta}$ and $l \gets 0$
\For{t = 1,2, \dots, T} 
  \State $\mathbf{v}          \gets \boldsymbol{\phi_t} \boldsymbol{\Gamma} \mathbf{P}(x_t)$
  \State $u                   \gets \mathbf{v} \mathbf{v}'$
  \State $l                   \gets l + \text{log} u$
  \State $\boldsymbol{\phi_t} \gets \mathbf{v} / u$
\EndFor
\State \Return $l$
\end{algorithmic}

In addition to this scaling solution, we also introduce two parameter transformations in a similar vein to what we did for the independent mixture models. Because it has been shown that the performance of constrained optimizers such as R's `constrOptim` can degrade when the parameter optimums lie close to their boundaries of the parameter space, we instead apply the following transformations to $\boldsymbol{\Gamma}$ and $\boldsymbol{\lambda}$

- As shown in \eqref{eq:paramN2W} and \eqref{eq:paramW2N}, because the parameters $\lambda_i$ for a Poisson HMM are limited to be non-negative, we apply the log transformation to get the working parameters $\eta_i$, and then exponentiate them to obtain the natural parameters again.

- Because the rows of the $\boldsymbol{\Gamma}$ matrix must all sum to one and the individual $\gamma_{ij}$ must all be non-negative, we apply the following transformation. ^[For details see @zucchini [pp. 48-49]]

\begin{align}
  \begin{aligned}\nonumber
    \text{Natural to working } &  
    \tau_{ij} = \text{log} \Bigg ( \frac{\gamma_{ij}}{1 - \sum_{k:k \neq i} \gamma_{ik}} \Bigg ) & i = 1, \dots,m, j = 2, \dots ,m \\
    \text{Working to natural } &
    \gamma_{ij} = \frac{\rho_{ij}}{1 + \sum_{k:k \neq i} exp(\tau_{ik})} & i,j = 1, \dots,m \\
    \text{where } &
    \rho_{ij} =   
      \begin{cases}
        exp(\tau_{ij})  & \text{for } i \neq j \\
        1              & \text{for } i = j 
      \end{cases}
  \end{aligned}
\end{align}

#Estimation via the EM Algorithm

As an alternative to direct numerical maximisation, often times the Expectation-Maximisation meta-algorithm is employed. EM is known as a meta-algorithm because rather than prescribing an explicit set of steps, it provides a framework that needs to be customised depending on the context. In the context of HMM's it can be useful due to the fact that the likelihood is guaranteed to increase for each iteration, and also that derivatives are not required (e.g. an optimiser such as `nlm` in R is not required). On the downside however, more implementation effort is often required, and convergence can sometimes be slow to achieve.

##EM in General
EM in general is an iterative scheme to find maximum likelihood estimates of parameters when some of the data are missing. Therefore by treating the hidden states of an HMM as those missing data, we can calculate what is known as the complete data log likelihood. The CDLL is the log likelihood of the parameters based on the observed and missing data.

The iterative scheme is as follows:

- Choose the starting values of the parameters to be estimated.

- E-Step: Compute the conditional expectations of those functions of the
missing data that appear in the complete-data log-likelihood.

- M-step: Maximisation of the log-likelihood with respect to the set of
parameters to be estimated (the missing data are substituted by their
conditional expectation).

- Assess convergence (with respect to some criterion) and repeat the E and
M-steps until convergence is reached.

##EM for Hidden Markov Models
When applied in the context of an HMM, the CDLL is the log-likelihood of the observations $\mathbf{x}^{(T)}$ and the missing data $\mathbf{c}^{(T)}$ which is given by:

\begin{align}
  \begin{aligned}\nonumber
    \text{log} \Big ( Pr(\mathbf{x}^{(T)}, \mathbf{c}^{(T)}) \Big ) &  
    = \text{log} \Bigg ( \delta_{c_1} \prod_{t=2}^T \gamma_{c_{t-1},c_t} \prod_{t=1}^T p_{c_t}(x_t) \Bigg )\\
 &  = \text{log } \delta_{c_1} 
    + \sum_{t=2}^T \text{log }\gamma_{c_{t-1},c_t} 
    + \sum_{t=1}^T \text{log } p_{c_t}(x_t) 
  \end{aligned}
\end{align}

And then by introducing the following indicator variables:
\begin{align}
  \begin{aligned}\nonumber
    u_j(t) & = 1 \iff c_t = j \\
    v_{jk}(t) & = 1 \iff c_{t-1} = j \text{ and } c_t = k
  \end{aligned}
\end{align}

We get:
\begin{align}
  \begin{aligned}\label{eq:cddl}
    \text{log} \Big ( Pr(\mathbf{x}^{(T)}, \mathbf{c}^{(T)}) \Big ) &  
    = \underbrace{\sum_{j=1}^m u_j(1) \text{log } \delta_j }_\text{term 1}
    + \underbrace{\sum_{j=1}^m \sum_{k=1}^m \Bigg ( \sum_{t=2}^T v_{jk}(t)   \Bigg ) \text{log } \gamma_{jk} }_\text{term 2}
    + \underbrace{\sum_{j=1}^m \sum_{t=1}^T u_j(t) \text{log } p_j(x_t)  }_\text{term 3}
  \end{aligned}
\end{align}


This shows how with the use of the indicator variables $u_j(t)$ and $v_{jk}(t)$ the CDLL can partitioned into three terms that can be optimized separately. The first term depends only on the initial distribution, the second term depends only on the transition matrix, and the third term depends only on the parameters related to the state dependent distributions (Poisson, in our continuing example).

###E Step

The conditional expectations of the functions of the missing data $\mathbf{c}^{(T)}$ given the observations $\mathbf{x}^{(T)}$ must be computed, and hence the quantities $v_{jk}(t)$ and $u_j(t)$ would be replaced by them:

\begin{equation}\label{eq:u}
\hat{u}_j(t) = Pr(C_t=j | \mathbf{x}^{(T)})
\end{equation}

\begin{equation}\label{eq:v}
\hat{v}_{jk}(t) = Pr(C_{t-1}=j, C_t =k | \mathbf{x}^{(T)})
\end{equation}

In calculating $Pr(C_t=j | \mathbf{x}^{(T)})$ we can use the previously calculated values $L_T$ \eqref{eq:likelihood} and $\boldsymbol{\alpha_t}$ \eqref{eq:forwardProb} along with a newly introduced backward probability vector $\boldsymbol{\beta_t}$ which is defined as follows:

\begin{align}
  \begin{aligned}\label{backwardProb}
    \boldsymbol{\beta_t}' & = \boldsymbol{\Gamma} \mathbf{P}(x_{t+1}) \boldsymbol{\Gamma} \mathbf{P}(x_{t+2})  \dots  \boldsymbol{\Gamma} \mathbf{P}(x_T) \mathbf{1}' \\
    & = \Bigg ( \prod_{s=t+1}^T  \boldsymbol{\Gamma} \mathbf{P}(x_s) \Bigg) \mathbf{1}'
  \end{aligned}
\end{align}

The reason for the name “backward probabilities” is that a recursion backward in time is used to calculate $\boldsymbol{\beta_t}$

We now state the following propositions:
^[Proved in @zucchini [pp. 60-63]]

\begin{equation}\nonumber
  \alpha_t(j) = Pr(\mathbf{X}^{(t)} = \mathbf{x}^{(t)}, C_t = j)
\end{equation}

\begin{equation}\nonumber
  \beta_t(i) = Pr(\mathbf{X}_{t+1}^{T} = \mathbf{x}^{(t)} | C_t = i)
\end{equation}

\begin{align}
  \begin{aligned}\label{eq:alphaBeta}
  \alpha_t(i) \beta_{t}(i) & = Pr(\mathbf{X}^{(T)} = \mathbf{x}^{(T)}, C_t = i) \\
    \text{and consequently } \boldsymbol{\alpha_t} \boldsymbol{\beta_t}' & = Pr(\mathbf{X}^{(T)} = \mathbf{x}^{(T)}) = L_T
  \end{aligned}
\end{align}

And now finally:

\begin{align}
  \begin{aligned}\nonumber
  \text{For } t & = 1, \dots , T, \\
  Pr(C_t = j | \mathbf{X}^{(T)} = \mathbf{x}^{(T)}) & = \alpha_t(j) \beta_{t}(j) / L_T \\
  \text{For } t & = 2, \dots , T, \\
  Pr(C_{t-1} = j, C_t = k | \mathbf{X}^{(T)} = \mathbf{x}^{(T)}) & = \alpha_{t-1}(j) \gamma_{jk} p_k(x_t) \beta_{t}(k) / L_T 
  \end{aligned}
\end{align}

With these steps in place we are now in a position to compute the conditional expectations \eqref{eq:u} and \eqref{eq:v} which replace $u_j(t)$  and $v_{jk}(t)$ respectively in the CDDL \eqref{eq:cddl}:

\begin{equation}\label{eq:uHat}
\hat{u}_j(t) = Pr(C_t=j | \mathbf{x}^{(T)}) = \alpha_t(j) \beta_{t}(j) / L_T
\end{equation}

\begin{equation}\label{eq:vHat}
\hat{v}_{jk}(t) = Pr(C_{t-1}=j, C_t =k | \mathbf{x}^{(T)}) = \alpha_{t-1}(j) \gamma_{jk} p_k(x_t) \beta_{t}(k) / L_T 
\end{equation}


###M Step
  
After the E step is complete, we now follow the M Step by maximising the CDDL \eqref{eq:cddl} with respect to the three sets of parameters of interest, $\boldsymbol{\delta}$, $\boldsymbol{\Gamma}$ and the parameters of the state-dependent distributions, namely $\lambda_1$,...,$\lambda_m$.\marginnote{We replace $p_j(x_t)$ in the CDDL with $e^{-\lambda_j} \lambda_j^x / x!$}

\begin{align}
  \begin{aligned}\nonumber
    \text{Term to maximise} & \qquad \text{Parameter} & \text{Solution} \\
    \sum_{j=1}^m \hat{u}_j(1) \text{log } \delta_j & \qquad \boldsymbol{\delta} & \delta_j = \frac{\hat{u}_j(1)}{\sum_{j=1}^m \hat{u}_j(1)} = \hat{u}_j(1)\\
    \sum_{j=1}^m \sum_{k=1}^m \Bigg ( \sum_{t=2}^T \hat{v}_{jk}(t)   \Bigg ) \text{log } \gamma_{jk} & \qquad \boldsymbol{\Gamma} & \gamma_{jk} = \frac{\sum_{t=2}^T \hat{v}_{jk}(t)}{\sum_{k=1}^m \sum_{t=2}^T \hat{v}_{jk}(t)} \\
    \sum_{j=1}^m \sum_{t=1}^T \hat{u}_j(t) \text{log } e^{-\lambda_j} \lambda_j^x / x!  & \qquad \lambda_1,\dots,\lambda_m & 0 = \sum_{t=1}^T \hat{u}_j(t) (-1 + x_t / \lambda_j) \\
    & & \text{ that is, by } \hat{\lambda}_j = \frac{\sum_{t=1}^T \hat{u}_j(t) x_t}{\sum_{t=1}^T \hat{u}_j(t)}
  \end{aligned}
\end{align}

