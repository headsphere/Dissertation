---
title: "Chapter 2"
header-includes:
   - \usepackage[]{algpseudocode}
output:
  rmarkdown::tufte_handout: default
  word_document: default
bibliography: library.bib
csl: harvard1.csl
---

```{r echo=FALSE}
options(digits = 3)

PlotTsAndDensity <- function (x) {
  #par(mfrow = c(1, 2))
  plot.ts(x, xlab = NULL, ylab = NULL)
  hist(x, breaks=30, freq = F, main=NULL, xlab = NULL, ylab = NULL)
  lines(density(x, na.rm = T, from = 0, to = max(x, na.rm = T)))
}
```

# HMM Model Derivation

## Independent Mixture Model

This section will outline the derivation of the HMM for univariate poisson observed data. We will use a toy example in order to illustrate the derivation.

An independent mixture model consists of $m$ component distributions with probability functions $p_i$ for $i \in {1, ... , m}$ and a “mixing distribution”. The mixing is performed by a discrete random variable $C$:
$$C =
  \begin{cases}
   1        & \text{with probability } \delta_1 \\
   \vdots   & \vdots \\
   i        & \text{with probability } \delta_i \\
   \vdots   & \vdots \\
   m        & \text{with probability } \delta_m = \sum_{i=1}^{m-1} \delta_i
  \end{cases}$$

Thus $Pr(C=i)=\delta_i$ must obey $0 < \delta_i < 1$ and that  $\sum_{i=1}^m \delta_i = 1$

For a discrete random variable X described by a mixture model consisting of $m$ components it holds that:

$$p(X) = \sum_{i=1}^m \delta_i p_i(X) \implies PR(X = x) = \sum_{i=1}^m Pr(X = x|C = i)$$

To estimate the parameters $\theta$ of the mixture distribution via ML estimation we maximize the combined likelihood of the components:

$$L(\theta_1,...,\theta_m,\delta_1,...,\delta_m|x_1,...,x_n) = \prod_{j=1}^n \sum_{i=1}^m \delta_i p_i(x_j, \theta_i)$$

For the example where $m = 2$ and the component distributions are poisson with parameters $\lambda_1$ and $\lambda_2$ the likelihood equation is as follows:

$$L(\lambda_1,\lambda_2,\delta_1,\delta_2 | x_1,...,x_n) = \prod_{j=1}^n \Bigg( \delta_1 \frac{\lambda_1^{x_j} e^{-\lambda_1}}{x_j !} + \delta_2 \frac{\lambda_2^{x_j} e^{-\lambda_2}}{x_j !} \Bigg)$$

In R this could be implemented naively as follows:

```{r}
lambdas = c(3, 15)
deltas = c(0.3, 0.7)
cdf = cumsum(deltas)
unifvec = runif(100)
d1 = rpois(sum(unifvec < cdf[1]),lambdas[1])
d2 = rpois(sum(unifvec >= cdf[1]),lambdas[2])
x = c(d1,d2) 
```

This code generates a series of x values according to a mixture of two poisson distributions with $\lambda_1=`r lambdas[1]`, \lambda_2=`r lambdas[2]`, \delta_1=`r deltas[1]`$ and $\delta_2=`r deltas[2]`$

We can see from both the following plots of the time series and the histograms (with fitted density) the data appears to conform to two separate distributions, or states:  

```{r echo=FALSE, fig.fullwidth = FALSE}
PlotTsAndDensity(x)
```

Before maximisiming the negative log-likelihood, the final step is that we must apply a transformation of the parameters in order to fulfill the requirements that $\sum_i \delta_i = 1$, $\delta_i > 0$ and $\lambda_i > 0$ for all $i$. This is achieved by log-transforming $\lambda$ and logit transforming $\delta$.

\begin{align}
 \begin{aligned}\label{eq:paramN2W}
    \text{Log-transform } \quad   & \eta_i = log \lambda_i, i = 1,...,m \\
    \text{Logit-transform } \quad & \tau_i = log \Bigg( \frac{\delta_1}{1 - \delta_2}\Bigg)
  \end{aligned}
\end{align}

In R:
```{r eval=TRUE}
logit <- function(delta2) log(delta2/(1-delta2))

eta = log(lambdas)
tau = logit(deltas[2])
```

After the maximisation has occured the original parameters can be recovered by the inverse transformations:

\begin{align}
 \begin{aligned}\label{eq:paramW2N}
  \text{exp-transform }\quad &  \lambda_i = e^{\eta_i}, i = 1,...,m \\
  \text{inverse logit }\quad &  \delta_2 = \frac{e^{\tau}}{1 + e^{\tau}}
  \end{aligned}
\end{align}

and additionally $\delta_1 = 1 - \delta_2$

```{r eval=TRUE}
invlogit <- function(tau) exp(tau)/(1+exp(tau))

lambdas = exp(eta)
delta2 = invlogit(tau)
delta1 = 1 - delta2
```

Now we have all the building blocks to maximise the negative log-likelihood for a two-state mixture distribution:

```{r warning=FALSE, cache=TRUE}
f <- function(theta,data) {
  eta <- theta[1:2]
  tau <- theta[3]
  lambdas = exp(eta) 
  delta2 = invlogit(tau)
  delta1 = 1-delta2

  L = delta1*dpois(data,lambdas[1]) + delta2*dpois(data,lambdas[2])
  -sum(log(L))
}

#use the 25% and 75%  quantiles of the data as a guess for lambdas
quantiles <- quantile(x)
lambdaGuess = c(quantiles[2],quantiles[4]) 

deltaGuess = 0.1

# Optimize using nlm
theta <- c(log(lambdaGuess), logit(deltaGuess))
res = nlm(f, theta, data = x)

# Back transform results
lambda1Hat = exp(res$estimate[1])
lambda2Hat = exp(res$estimate[2])

delta2Hat = invlogit(res$estimate[3])
delta1Hat = 1-delta2Hat
```

Estimated values:

$$\boldsymbol{\lambda} = 
\begin{pmatrix}
 `r lambdas[1]` \\
 `r lambdas[2]`
\end{pmatrix} \qquad
\boldsymbol{\hat{\lambda}} = 
\begin{pmatrix}
 `r lambda1Hat` \\
 `r lambda2Hat`
\end{pmatrix}$$

$$\boldsymbol{\delta} = 
\begin{pmatrix}
 `r deltas[1]` \\
 `r deltas[2]`
\end{pmatrix} \qquad
\boldsymbol{\hat{\delta}} = 
\begin{pmatrix}
 `r delta1Hat` \\
 `r delta2Hat`
\end{pmatrix}$$

The mixture distribution parameters appear to be estimated satisfactorily.

The next step is to introduce into the model a way of capturing the transition behaviour between the states over time.

## Markov switching process

The independent mixture model is used to cater for the overdispersion noticed in some time series data. The second feature often found is that they exhibit serial dependece between the observations. This dependence is identified via the autocorrelation function of the data. To cater for this we model the dependence between the mixtures by introducing a Markov process. With this building block we will have completely defined the hidden Markov model. 

If we define $\{C_t : t = 1,2, \dots \}$ as the unbserved hidden states, and $\{X_t : t = 1,2, \dots \}$ as the observed data points, we can say a hidden Markov is a dependent mixture where:

$$Pr(C_t = i | C^{(t-1)}) = Pr(C_t = i | C_{t-1}), t = 2,3,\dots$$
$$Pr(X_t = x | X^{(t-1)}, C^{(t)}) = Pr(X_t = x | C_t), t \in \mathbb{N}$$

As shown in the directed graph below, this is saying that when $C_t$ is known the distribution of $X_t$ only depends on $C_t$

\begin{figure}
  \includegraphics{images/hmm-states}
\end{figure}

For discrete distributions we can define the m-state dependant distribution function as:

\begin{equation} \label{eq:dependantDist}
  p_i(X) = Pr(X_t = x | C_t = i), i,2,\dots,m
\end{equation}

Which is simply the conditional probability of observing $x$ at time $t$ when $C$ is in state $i$.

Additionally we can define the unconditional probability of being in state $i$ at time $t$ as:

\begin{equation} \label{eq:stateDist}
  u_i(t) = Pr(C_t = i), t = 1,\dots,T
\end{equation}

We will often want to calculate the marginal distribution of $X$. This can be achieved by summing over the states and using the hidden state $\eqref{eq:stateDist}$ and state-dependent $\eqref{eq:dependantDist}$ distributions:

\begin{align}
 \begin{aligned}\nonumber  
    Pr(X_t = x) &= \sum_{i=1}^m Pr(C_t = i) Pr(X_t = x | C_t = i) \\
                &= \sum_{i=1}^m u_i(t) p_i(x) \\
                &= \begin{pmatrix}
                     u_1(t),\dots,u_m(t) 
                   \end{pmatrix}
                   \begin{pmatrix}
                     p_1(x) &        & 0 \\
                            & \ddots &   \\
                     0      &        & p_m(x) 
                   \end{pmatrix}
                   \begin{pmatrix}
                     1 \\
                     \vdots \\
                     1 
                   \end{pmatrix} \\
                &= \mathbf{u}(t) \mathbf{P}(x) \mathbf{1'}
 \end{aligned}
 \end{align}
 
$\mathbf{u}(t)$ is the distribution of the hidden states, and $\mathbf{P}(x)$ is the state-dependent distribution of the observations. $\mathbf{u}(1)$ is defined as the initial distribution of the Markov chain.

The diagonal matrix of conditional probabilities ${P}(x)$ defined as:
$$\mathbf{P}(x) = \begin{pmatrix}
                   p_1(x) &        & 0 \\
                          & \ddots &   \\
                   0      &        & p_m(x) 
                 \end{pmatrix}$$

Where for example: $\boldsymbol{\lambda} = \begin{pmatrix}
                           `r lambdas[1]` \\
                           `r lambdas[2]`
                         \end{pmatrix}$
$$\mathbf{P}(1) = \begin{pmatrix}
 \frac{e^{-`r lambdas[1]`} `r lambdas[1]`^1}{1!} & 0 \\
 0                     & \frac{e^{-`r lambdas[2]`} `r lambdas[2]`^1}{1!} 
\end{pmatrix} = 
\begin{pmatrix}
 `r dpois(1, lambdas[1])` & 0 \\
 0                        & `r dpois(1, lambdas[2])`
\end{pmatrix}$$

This can be implemented easily in R as:

```{r}
PMat <- function(x) diag(dpois(x = x, lambda = lambdas))

PMat(1)
```

Next we introduce the transition probability matrix $\boldsymbol{\Gamma}(t)$ which is the $m \times m$ matrix of transition probabilities at state $t$:

$$\boldsymbol{\Gamma}(1) = \begin{pmatrix}
                             \gamma_{11} & \cdots & \gamma_{1m} \\
                             \vdots      & \ddots & \vdots  \\
                             \gamma_{m1} & \cdots & \gamma_{mm}
                           \end{pmatrix}$$

$$\gamma_{ij}(t) = Pr(C_{s+t} = j | C_s = i)$$

Because the Markov chain satisfies the Chapman-Kolmogorov equations we can say that $\mathbf{u}(t) = \mathbf{u}(1) \boldsymbol{\Gamma}^{t-1}$ (where $\boldsymbol{\Gamma}$ is shorthand for $\boldsymbol{\Gamma}(1)$) and hence it follows that: 

\begin{equation} \label{eq:marginalNonStat}
  Pr(X_t = x) = \mathbf{u}(1) \boldsymbol{\Gamma}^{t-1} \mathbf{P}(x) \mathbf{1'}
\end{equation}

If the Markov chain can be shown to be stationary with stationary distribution $\boldsymbol{\delta}$, because $\boldsymbol{\delta} \boldsymbol{\Gamma}^{t-1} = \boldsymbol{\delta}$ for all $t$ we get:

\begin{equation} \label{eq:marginalStat}
  Pr(X_t = x) = \boldsymbol{\delta} \mathbf{P}(x) \mathbf{1'}
\end{equation}

It can be shown that if the stationary distribution exists then $\boldsymbol{\delta}$ can be found using the following calculation:

\begin{equation} \label{eq:stationary}
  \boldsymbol{\delta} (\mathbf{I}_m - \boldsymbol{\Gamma} + \mathbf{U}) = \mathbf{1}
\end{equation}

Where $\mathbf{I}_m$ is an $m \times m$ identity matrix and $\mathbf{U}$ is an $m \times m$ matrix of ones. This can be implemented simply in R as follows:


```{r}
statdist <- function(gamma){
  m = dim(gamma)[1]
  matrix(1,1,m) %*% solve(diag(1,m) - gamma + matrix(1,m,m))
}
```

To illustrate this concept, lets extend the example we used previously this time introducing a hypothetical transition matrix:

```{r}
gamma = matrix(c(0.9,0.1,0.4,0.6), nrow = 2, byrow = TRUE)
```

$$\boldsymbol{\lambda} = \begin{pmatrix}
                           `r lambdas[1]` \\
                           `r lambdas[2]`
                         \end{pmatrix} \qquad
\boldsymbol{\Gamma} = \begin{pmatrix}
                        `r gamma[1,1]` & `r gamma[1,2]` \\
                        `r gamma[2,1]` & `r gamma[2,2]`
                      \end{pmatrix}$$

We can now calculate the stationary distribution $\boldsymbol{\delta}$ using equation $\eqref{eq:stationary}$:

```{r}
delta = statdist(gamma)
```

$$\boldsymbol{\delta} = \begin{pmatrix}
                           `r delta[1]` \\
                           `r delta[2]`
                         \end{pmatrix} \qquad$$
                         
With which we can now calculate the marginal distribution $\eqref{eq:marginalStat}$, for example for $P(X = 1)$:

```{r}
x = 1
sum(delta %*% PMat(x))
```

## Calculating the Likelihood

The first step to calculate the likelihood function is to extend the univariate marginal distribution $\eqref{eq:marginalNonStat}$ to a bivariate distribution. @zucchini show that $\eqref{eq:marginalNonStat}$ can be extended so that:

$$Pr(X_t = v, X_{t+k} = w) = \mathbf{u}(t) \mathbf{P}(v) \boldsymbol{\Gamma}^k \mathbf{P}(w) \mathbf{1'}$$

While the equivalent for a stationary chain reduces to:

$$Pr(X_t = v, X_{t+k} = w) = \boldsymbol{\delta} \mathbf{P}(v) \boldsymbol{\Gamma}^k \mathbf{P}(w) \mathbf{1'}$$

This then can be generalised to a $T$-th order distribution and hence the likelihood for a set of observations $\mathbf{X}^{(T)} = (X_1,X_2,\dots,X_T)$ given the parameters of an HMM is:

$$L_T = \boldsymbol{\delta} \mathbf{P}(x_1) \boldsymbol{\Gamma} \mathbf{P}(x_2) \dots \boldsymbol{\Gamma} \mathbf{P}(x_T) \mathbf{1'}$$

A recursive algorithm can be defined to calculate the likelihood, first by defining the foward probability vector:

$$\boldsymbol{\alpha_t} = \boldsymbol{\delta} \mathbf{P}(x_1) \boldsymbol{\Gamma} \mathbf{P}(x_2) \dots \boldsymbol{\Gamma} \mathbf{P}(x_T)$$

From which the likelihood can now be recursively calculated:

\begin{align}
 \begin{aligned}\nonumber  
     \boldsymbol{\alpha_1} &= \boldsymbol{\delta} \mathbf{P}(x_1) \\
     \boldsymbol{\alpha_t} &= \boldsymbol{\alpha_{t-1}} \boldsymbol{\Gamma} \mathbf{P}(x_t) \qquad for \quad t = 2,3,\dots,T\\
     L_T                   &= \boldsymbol{\alpha_T} \mathbf{1'}
 \end{aligned}
 \end{align}

If $\boldsymbol{\delta}$ provided is in fact the stationary distribution of the Markov chain, then the recursive scheme becomes:

\begin{align}
 \begin{aligned}\nonumber  
     \boldsymbol{\alpha_0} &= \boldsymbol{\delta} \\
     \boldsymbol{\alpha_t} &= \boldsymbol{\alpha_{t-1}} \boldsymbol{\Gamma} \mathbf{P}(x_t) \qquad for \quad t = 1,2,\dots,T\\
     L_T                   &= \boldsymbol{\alpha_T} \mathbf{1'}
 \end{aligned}
 \end{align}
 
 With R implementation:

```{r}
pois.hmm.alpha <- function(x, gamma, delta = NULL)
{
  n = length(x)
  alpha = vector(mode="list", length=n)
  alpha0= statdist(gamma)
  
  for(t in 1:n)
  {
    if(t==1)
    {
      prevAlpha = alpha0
    }
    else
    {
      prevAlpha = alpha[[t-1]]
    }
    alpha[[t]] = prevAlpha %*% gamma %*% PMat(x[t])
  }
  return(alpha[[length(x)]])
}
```

The problem with this scheme however is that as $t$ increases, $\boldsymbol{\alpha_t}$ rapidly diminishes as it is the product of a large number of probabilities, and when implemented computationally, will be rounded to zero (a problem known as underflow). We can show that the calculation works for small $t$ however needs to be re-worked for any non-trivial time series.

As an example we can calculate $Pr(X_1 = 0, X_2 = 2, X_3 = 1)$, using the previously defined $\boldsymbol{\Gamma}$ and $\boldsymbol{\lambda}$ parameters: 
```{r}
x = c(0,2,1)
alpha = pois.hmm.alpha(x, gamma)
prob = sum(alpha)
prob
```
@durbin [pp.99] present a method of scaling the computations to mitigate this issue of underflow. In summary they propose the following algorithm (where $\boldsymbol{\Gamma}$ and $\mathbf{P}(x_t)$ are $m \times m$ matrices, $\mathbf{v}$ and $\boldsymbol{\phi_t}$ are vectors of length $m$, $u$ is a scalar, and $l$ is the scalar which accumulates the log-likelihood).

\begin{algorithmic}
\State set $\phi_0 \gets \boldsymbol{\delta}$ and $l \gets 0$
\For{t = 1,2, \dots, T} 
  \State $\mathbf{v}          \gets \boldsymbol{\phi_t} \boldsymbol{\Gamma} \mathbf{P}(x_t)$
  \State $u                   \gets \mathbf{v} \mathbf{v}'$
  \State $l                   \gets l + \text{log} u$
  \State $\boldsymbol{\phi_t} \gets \mathbf{v} / u$
\EndFor
\State \Return $l$
\end{algorithmic}

In addition to this scaling solution, we also introduce two parameter transformations in a similar vein to what we did for the independent mixture models. Because it has been shown that the performance of constrained optimizers such as R's `constrOptim` can degrade when the parameter optimums lie close to their boundaries of the parameter space, we instead apply the following transformations to $\boldsymbol{\Gamma}$ and $\boldsymbol{\lambda}$

- As before \eqref{eq:paramN2W}, \eqref{eq:paramN2W}, because the parameters $\lambda_i$ for a Poisson HMM are limited to be non-negative, we apply the log transformation to get the working parameters $\eta_i$, and then exponentiate them to obtain the natural parameters again.

- Because the rows of the $\boldsymbol{\Gamma}$ matrix must all sum to one and the individual $\gamma_{ij}$ must all be non=-negative, we apply the following transformation. For details see @zucchini [pp. 48-49]:

\begin{align}
  \begin{aligned}\nonumber
    \text{Natural to working } &  
    \tau_{ij} = \text{log} \Bigg ( \frac{\gamma_{ij}}{1 - \sum_{k:k \neq i} \gamma_{ik}} \Bigg ) & i = 1, \dots,m, j = 2, \dots ,m \\
    \text{Working to natural } &
    \gamma_{ij} = \frac{\rho_{ij}}{1 + \sum_{k:k \neq i} exp(\tau_{ik})} & i,j = 1, \dots,m \\
    \text{where } &
    \rho_{ij} =   
      \begin{cases}
        exp(\tau_{ij})  & \text{for } i \neq j \\
        1              & \text{for } i = j 
      \end{cases}
  \end{aligned}
\end{align}

##Estimation via the EM Algorithm

As an alternative to direct numerical maximisation, often times the Expectation-Maximisation meta-algorithm is employed. EM is known as a meta-algorithm because rather than prescribing an explicit set of steps, it provides a framework that needs to be customised depending on the context. In the context of HMM's it can be useful due to the fact that the likelihood is guaranteed to increase for each iteration, and also that derivatives are not required (e.g. an optimiser such as nlm in R is not required). On the downside however, more implementation effort is often required, and convergence can sometimes be slow to achieve.

EM in general is an iterative scheme to find maximum likelihood estimates of parameters when some of the data are missing. Therefore by treating the hidden states of an HMM as those missing data, we can calculate what is known as the complete data log likelihood. The CDLL is the log likelihood of the parameters based on the observed and missing data

The iterative scheme is as follows:

- Choose the starting values of the parameters to be estimated

- E-Step: Compute the conditional expectations of those functions of the
missing data appear in the complete-data log-likelihood.

- M-step: Maximisation of the log-likelihood with respect to the set of
parameters to be estimated (the missing data are substituted by their
conditional expectation).

- Assess convergence (with respect to some criterion) and repeat the E and
M-steps until convergence is reached.

When applied to the context of an HMM, the CDLL is the log-likelihood of the observations $\mathbf{x}^{(T)}$ and the missing data $\mathbf{c}^{(T)}$ which is given by:

\begin{align}
  \begin{aligned}\nonumber
    \text{log} \Big ( Pr(\mathbf{x}^{(T)}, \mathbf{c}^{(T)}) \Big ) &  
    = \text{log} \Bigg ( \delta_{c_1} \prod_{t=2}^T \gamma_{c_{t-1},c_t} \Bigg ) 
  \end{aligned}
\end{align}


  
\pagebreak

#References