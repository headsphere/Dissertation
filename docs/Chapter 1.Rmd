---
header-includes:
   - \usepackage[]{algorithmicx}
---
\chapter{Introduction}

#Problem Background

In the world of high frequency trading, market makers constitute the majority of the players. Their role in the markets comprises of providing liquidity to position takers by making passive orders. Instead of taking directional bets on financial asset, these market makers instead attempt to profit by making tiny margins on a huge number of trades. As passive providers of liquidy they often are subject to the risk of adverse selection. Adverse selection is often defined as the "natural tendency for passive orders to fill quickly when they should fill slowly, and slowly (or not at all) when they should fill quickly" [@Jeria2008]. Order flow is considered toxic when informed traders take advantage of uninformed traders, and these market makers are adversely selected often without even realising it. 

The most notable proxy for information assymetry (and hence order toxicity) is the Probability of Information-Based Trading (PIN) developed by @Easley1996a. @Easley2012 then re-examined PIN, in order to update it for a high-frequency context, and hence developed Volume-Synchronised PIN (VPIN). The critical difference between PIN and VPIN is in the methodology employed to categorise trades as either buys or sells; where PIN uses intervals based off clock-time, VPIN instead uses intervals based off volume-time. At their essence however both are proxy measures of trade imbalances.

Historically PIN has been used as a metric to measure the extent of order flow toxicity. If order flow becomes too toxic, market makers are forced out of the market. As they withdraw, liquidity disappears, which increases even more the concentration of toxic flow in the overall volume, which then triggers a feedback mechanism that forces even more market makers out. This cascading effect has caused  liquidity-induced crashes in the past, the 2010 Flash Crash being one (major) example of it. One hour before the flash crash, order flow toxicity was at historically high levels relative to recent history. ELO claim that using the VPIN metric, this crash could have been predicted one hour before it actually happened.

This metric has been proposed as a new standard for market makers (along with financial regulators) as it enables them to set appropriate bid-ask prices such that the spread compensates them for the probability of dealing with an insider. ELO state that this is not to be a replacement for the CBOE Volatility Index (VIX) but rather as a complement to it.

VPIN's theory is consistent with the anecdotal evidence reported by a joint SEC-CFTC study on the events of May 6, 2010 [@SEC2010]. Given the relevance of these findings, the SEC requested an independent study to be carried out by the Lawrence Berkeley National Laboratory. This Government laboratory concluded:

>This [VPIN] is the strongest early warning signal known to us at this time.
>
>[@Bethel2011]
>

The goal of this paper is to validate the predictive power of the VPIN metric, and to explore the relationship between PIN and VPIN by way of a Hidden Markov Model (HMM) to estimate model parameters and decoded hidden market states.

#Model Background

First we explain the background to the PIN model and its relationship with VPIN. Traditionally PIN is explained by way of a sequential trade model. This is as follows:

Denote a security's price as $S$. Its present value is $S_1$. Once a certain amount of new information has been incorporated into the price, S will be either $S_B$ (bad news) or $S_G$ (good news). There is a probability $\alpha$ that new information will arrive within the time-frame of the analysis, and a probability $\delta$ that the news will be bad  (i.e., $1 - \delta$ that the news will be good). Traders are classified into two types: So-called noise, or liquidity traders, are those with no information based reason to trade. Their arrivals are measured as a Poisson process with rate $\epsilon$. Information-based traders on the other hand are those that are trading based off some private information that has not been priced into the asset. They are also modelled as a Poisson process, with arrival rate $\mu$. 

If an information event arrives and it is a 'good news' event, both uninformed and informed traders buy, while only uninformed traders sell. Conversely if a 'bad news' information event occurs only uninformed traders buy, while both informed and uninformed traders sell. If no information event occurs then only uninformed traders are in the market. The model is visually explained in the decision tree in Figure 1.  

\begin{figure*}[h]
  \includegraphics{images/sequential_trade_model}
\end{figure*}

The original PIN model requires the estimation of four non-observable parameters, namely $\alpha$, $\delta$, $\mu$, and $epsilon$. This was originally done numerically via Maximum likelihood on the observed number of buys and sells, through the fitting of a mixture of three Poisson distributions. VPIN can in some regards be considered a high-frequency estimate of PIN which takes into account the time-varying nature of the data. Instead of calculating liquidity based off clock-time, VPIN uses what is called volume-time, whereby trades are placed into equally sized buckets each with a uniform amount of volume. The intuition behind this is that in a high-frequency environment, these volume bars will be spaced out widely when there is little information-based trading, and packed tightly when there is a higher amount. The idea is that the more relevant is a piece of information, the more volume it will attract. Within each of these bars the number of trades that are buys or sells are inferred, and then VPIN is calculated as the degree of imbalance between buys and sells. If we have $\tau = 1,2,... n$ volume buckets, within each bucket trades are classified as buys $V_{\tau}^B$, and sells $V_{\tau}^S$ where $V = V_{\tau}^B + V_{\tau}^S$ for each $\tau$. Using this approach, the parameters can be estimated anlytically instead of numerically.

A crucial part of this calculation involves the classification of trades into buys and sells, as traditional order-book data sources do not include the trade direction. Rather than using the Tick-rule, @Lee1991 or other trade classification techniques, ELO propose a new volume classification method called Bulk Volume Classification^[The algorithm to compute the VPIN metric is in Appendix A]. This departs from standard trade classification schemes in two ways: First, volume is classified in bulk, and second this methodology probabilistically classifies part of a bar's volume as buy, and the remainder as sell. Empirical studies have shown Bulk Volume Classification to be more accurate than the Tick-rule, despite of not requiring level-1 tick data (only bars) [@Easley2012c]. Within a volume bucket, the amount of volume classified as buy is:

\begin{equation} \label{eq:vol_buy}
V_{\tau}^B = \sum_{i = t(\tau - 1) + 1}^{t(\tau)} V_i Z \left(\frac{S_i - S_{i-1}}{\sigma_{\Delta S}}\right)
\end{equation}

where $t(\tau)$ is the index of the last (volume or time) bar included in bucket $\tau$, $V_{\tau}^B$ is the buy volume (traded against the Ask), $V_i$ is the total volume per bucket, $Z$ is the Standard Normal Distribution, and $\sigma_{\Delta S}$ is the standard deviation of price changes betwenen (volume or time) bars. Because all buckets contain the same amount of volume $V$, 

\begin{equation} \label{eq:vol_sell}
V_{\tau}^S = \sum_{i = t(\tau - 1) + 1}^{t(\tau)} V_i \left( 1 - Z \left(\frac{S_i - S_{i-1}}{\sigma_{\Delta S}} \right) \right) = V - V_{\tau}^B
\end{equation}

The dynamics of buys and sells are driven by the sequential trade model parameters mentioned previously ($\alpha$, $\delta$, $\mu$ and $\epsilon$). The expected arrival rate of informed trade becomes: $E[V_{\tau}^B - V_{\tau}^S] = \alpha \mu(2 \delta - 1)$ and the absolute expected values for sufficiently large $\mu$ is $E[|V_{\tau}^B - V_{\tau}^S|] \approx \alpha \mu$.

The total expected arrival rate is:

$$\frac{1}{n}\sum_{\tau = 1}^n (V_{\tau}^B + V_{\tau}^S) = V = $$

$$\underbrace{\alpha (1 - \delta) (\epsilon + \mu + \epsilon) }_\text{Volume from good news} + \underbrace{\alpha \delta (\mu + \epsilon + \epsilon) }_\text{Volume from bad news} + \underbrace{(1 - \alpha) (\epsilon + \epsilon) }_\text{Volume from no news} = \alpha \mu + 2 \epsilon$$

VPIN is then calculated as:

$$VPIN = \frac{\alpha \mu}{\alpha \mu + 2 \epsilon} = \frac{\alpha \mu}{V} = \frac{\sum_{\tau = 1}^n (V_{\tau}^S - V_{\tau}^B)}{(2 \delta - 1)n V} \approx  \frac{\sum_{\tau = 1}^n |V_{\tau}^S - V_{\tau}^B|}{n V}$$

To simulate the volume bars across the course of a hypothetical trading day I generate data according to the following procedure: Generate a Bernouilli random variable with parameter $\alpha$ to determine whether an information event has ocurred within this volume bar. Then generate a second Bernouilli R.V. with parameter $\delta$ to determine whether the event was a 'good' or a 'bad' information event. According to the sequential trade model we then simulate the number of buys and sells for the volume bar using Possion R.V.s. This is repeated for the total number of expected volume bars for the trading days, then this hypothetical day can be simulated a large number of times in order to calculate an expected VPIN value. The code to generate Monte Carlo simulations of the volume bars has been outlined in Appendix B.  

This approach is not without its detractors however, and a long-running dispute has been simmering for the past several years with counterclaims attacking the viability of both the bulk-classification methodology [@Andersen2012a] and the predictive power of VPIN itself[@Andersen2014]. Partly due to this controversy we believe that this is a topic worth investigating further.  

We shall follow the approach postulated by @Yin2014 in which the time bars are classified as buys and sells, and then analysed as a bivariate Poisson HMM. After fitting an appropriate HMM we shall be able to conduct a decoding exercise to find the most likely hidden market state at any given point in time. The purpose of this is then to find some relationship with the corresponding volume bars and their subsequent VPIN values.

We conduct this analysis on both simulated data and empirical data. The data we have acquired are the raw tick-by-tick trade and quote data from the NYSE for the S&P 500 SPDR ETF for the day of the Flash Crash, 6 May 2010. 