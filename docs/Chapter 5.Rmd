\chapter{Empirical Data Analysis}
```{r message=FALSE, echo=FALSE}
library(ggplot2)
library(reshape2)
library(xts)
library(xtable)
options(xtable.comment = FALSE)
options(xtable.booktabs = TRUE)
options(digits=3)
```

#The Data

In order to validate the assumptions of the preceding chapters the next step was to run an empirical data analysis. The data we chose to analyse was the SPDR S&P 500 exchange-traded fund (ticker symbol SPY) as this one one of the financial products that was most adversely affected in the May 6 2010 Flash Crash. The reason why we analyse SPY instead of the E-Mini S&P 500 futures contracts as per @Easley2011 is due to the fact that the data we had available was NYSE TAQ, and the E-Mini data would only have been available from the Chicago Mercantile Exchange. It is our hope that SPY will be a suitable proxy for the E-Mini.

On downloading the data from the NYSE TAQ database for ticker SPY for the 6th of May 2010, we found we had 1,549,379 trades and 8,343,441 quotes in raw CSV format. For various reasons, raw trade and quote data contains numerous data errors [@Brownlees2006]. Therefore, the data is not suited for analysis right-away and data-cleaning is an essential step in dealing with tick-by-tick data. Trade clean-up involved removing zero prices, limiting the observations to a single exchange, filtering by sale condition, and merging any records with the same timestamp. This was then followed by quote clean-up procedure which was responsible for removing any errant values with abnormally large spreads and outliers where the mid-quote deviated by more than 25 median absolute deviations from a rolling centred median. Once the data were cleaned up, then the trades and quotes were matched up using the technique as outlined in @Vergote2005. Finally because the data from the exchange doesn't report the trade direction, we employed the @Lee1991 approach to inferring whether individual trades were buys or sells. 

After this preprocessing, the consolidated trade and quote dataset now comprised of 791,173 entries, which by converting the data into one second time bars was then further compressed into 24,839 records, a dataset size that is much more tractable for in-memory analysis in R. The data are displayed visually in the following plots:

```{r dataload, cache=TRUE, echo=FALSE}
deser = readRDS("../data/SPY_BuysSells.RDS")
n <- length(deser[,1])
# trades = data.frame(Buckets=seq(1:n), Buy=coredata(deser)[,2], Sell=coredata(deser)[,3])
trades = data.frame(Time=index(deser), Price=coredata(deser)[,1], Buy=coredata(deser)[,2], Sell=coredata(deser)[,3])

df_melt = melt(trades, id.vars = 'Time')
```

```{r, fig.width = 10, fig.height = 7, fig.fullwidth = TRUE, echo=FALSE, fig.cap="Time series of price and inferred buys and sells"}
ggplot(df_melt, aes(x = Time, y = value, colour=variable)) +
  geom_line() +
  facet_wrap(~ variable, scales = 'free_y', ncol = 1)
```

#Analysis Procedures

A visual examination of the data show the clear downward spike in price, matching the known events that were known to happen between 2:32 and the subsequent 36 minutes. However there does not appear to be any untoward pattern in the number of buys and sells, apart from a marginal up-tick in activity after the crash, nothing that seems out of character when compared to the rest of the trading day.

```{r, fig.width = 10, fig.height = 4, fig.fullwidth = FALSE, echo=FALSE, fig.cap="Fitted density of inferred buys and sells (bandwidth=30, kernel=gaussian)"}
trades = data.frame(Time=index(deser), Buy=coredata(deser)[,2], Sell=coredata(deser)[,3])
df_melt = melt(trades, id.vars = 'Time')
ggplot(df_melt,aes(x=value, fill=variable)) + 
  stat_density(adjust = 30, alpha=0.25, kernel = 'g')
  # geom_density(alpha=0.25, kernel='epanechnikov', size='20') + 
  # facet_wrap(~ variable, scales = 'free_y', ncol = 1)
```

The fitted density of the buys and sells however lead us to potential issues with the modelling approach we have undertaken. First of all neither of the distributions appear to match a simple Poisson mixture distribution. It is however a possibility that the data observations from the lower tail constitute other component distributions for some underlying reason so far unexplained in this work. The second issue is that the multi-modal nature of our hypothesised model does not seem to apply with this empirical data.

```{r jump-analysis, cache=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.width = 10, fig.height = 8, fig.fullwidth = TRUE, echo=FALSE, fig.cap="Jump analysis", results='hide'}
source('../code/jump.R')
tradesMatrix <- as.matrix(cbind(trades$Buy, trades$Sell),ncol=2)
colnames(tradesMatrix) <- c("buy", "sell")
temp <- jump(tradesMatrix,y=c(1.5,2,2.5),rand=10,trace=F,plotjumps=FALSE)

cl <- kmeans(tradesMatrix, 3)
trades$cluster=factor(cl$cluster)
centers=as.data.frame(cl$centers)

```

As per the simulation study, we repeat the Jump analysis to find the optimum number of clusters. Unsurprisingly given our results up to this point, no clear number of clusters emerges, therefore we continue with the assumption that 3 is still the optimum number:

```{r, fig.width = 10, fig.height = 6, fig.fullwidth = FALSE, echo=FALSE, fig.cap="Inferred buys and sells with results of 3 fitted clusters from k-means procedure"}
ggplot(data=trades, aes(x=Buy, y=Sell, color=cluster )) +
  geom_point() +
  geom_point(data=centers, aes(x=buy,y=sell, color="Center")) +
  geom_point(data=centers, aes(x=buy,y=sell, color="Center"), size=52, alpha=.3, show_guide=FALSE)
```

Visualising the clusters to identify the centroids, again does not appear to be too helpful here as there are clearly no demarcated clusters in the dataset. The cluster centroids however are as follows:
```{r clustering, cache=TRUE, echo=FALSE}
clustersDf = data.frame(cbind(cl$centers, cl$size))
names(clustersDf) = c("Buy", "Sell", "Cluster Size")

attach(clustersDf)
ordered = clustersDf[order(-`Cluster Size`),]
epsilonBuy = ordered$Buy[1]
epsilonSell = ordered$Sell[1]
ordered = clustersDf[order(-Buy),]
lambdaBuy2 = ordered$Buy[1]
muBuy = lambdaBuy2 - epsilonBuy
ordered = clustersDf[order(-Sell),]
lambdaSell2 = ordered$Sell[1]
muSell = lambdaSell2 - epsilonSell
detach(clustersDf)
```
```{r results='asis', echo=FALSE}
xtable(clustersDf, caption = "Cluster centroids and sizes")
```


Given the centroids of the three fitted clusters (no matter how uninterpretable they may be) we repeat the analysis we conducted as part of the simulation study in order to identify initial starting values for the HMM EM estimation. Hence we can formulate the initial state parameter estimates:

$$\lambda_b = \begin{pmatrix}
\epsilon_b \\
\epsilon_b + \mu_b \\
\epsilon_b
\end{pmatrix} 
= \begin{pmatrix}
`r epsilonBuy` \\
`r lambdaBuy2` \\
`r epsilonBuy`
\end{pmatrix}\qquad
\lambda_s = \begin{pmatrix}
\epsilon_s \\
\epsilon_s \\
\epsilon_s + \mu_s 
\end{pmatrix}
= \begin{pmatrix}
`r epsilonSell` \\
`r epsilonSell` \\
`r lambdaSell2`
\end{pmatrix}$$

```{r HMM, cache=TRUE, echo=FALSE, results='hide'}
source('../code/hmmBivPois.R')

runEM <- function (lambda_buy, lambda_sell, trades) {
  m_buy = length(lambda_buy)
  m_sell = length(lambda_sell)
  mn <- m_buy * m_sell
  delta_buy = matrix(rep(1, mn), ncol = mn)
  delta_buy = delta_buy/apply(delta_buy, 1, sum)
  gamma <- matrix(rep(1,mn), nrow = mn, ncol = mn, byrow = TRUE)
  gamma = gamma/apply(gamma,1,sum)
  model <- bi.pois.HMM.EM(cbind(trades$Buy,trades$Sell),m_buy,m_sell,lambda_buy,lambda_sell,gamma,delta_buy)
}
lambda_buy = c(epsilonBuy, epsilonBuy + muBuy, epsilonBuy)
lambda_sell = c(epsilonSell,epsilonSell, epsilonSell + muSell)

model = runEM(lambda_buy, lambda_sell, trades)
m1 = runEM(mean(trades$Buy), mean(trades$Sell), trades)
m2 = runEM(c(20,40), c(20,40), trades)
```

After running the EM estimation procedure for $m=3$ we obtain the following fitted values: 

```{r, echo=FALSE, results='asis'}
lambdaDf <- data.frame(
  `Lambda Buy Hat`=model$lambda_buy, 
  `Lambda Sell Hat`=model$lambda_sell
  )

xtable(lambdaDf, caption = "Lambda Buy and Sell: Fitted arrival rates of informed and uninformed traders")
```
```{r, fig.width = 10, fig.height = 4, fig.fullwidth = TRUE, echo=FALSE, eval=TRUE}
mn <- length(model$lambda_buy) * length(model$lambda_sell)
states = bi.pois.HMM.local_decoding(cbind(trades$Buy,trades$Sell), mn, model$lambda_buy, model$lambda_sell,model$gamma)

# statesDf = data.frame(Time=trades$Time, `Decoded States`=states)
# df_melt = melt(statesDf, id.vars = 'Time')
# ggplot(df_melt, aes(x = Time, y = value)) +
#   geom_step() +
#   facet_wrap(~ variable, scales = 'free_y', ncol = 1)
```

On estimation of a fitted model, we are now able to calculate the decoded hidden states for our observed series of buys and sells. Plotting the states as a time series is not particularly illustrative, therefore we repeat the analysis we conducted in the simulation study and attempt to calculate volume bars from the time bars, calculate VPIN values for these volume bars and then regress these VPIN values against the decoded states.

```{r, echo=FALSE, results='hide', warning=FALSE, cache=TRUE, message=FALSE}
source('../code/VPIN-Timebars.R')

V = 55
n = length(states)
trades = data.frame(Buckets=seq(1:n), 
                    Buy=coredata(trades)[,2], 
                    Sell=coredata(trades)[,3],
                    States=states,
                    TimeBar=rep(as.POSIXct(NA,"",tz = "GMT"),n))
                    # TimeBar=index(trades))

volBars =  GenerateVolBars(trades, n)
```

A small sample of the generated volume bars from the empirical data is shown below: 
```{r, echo=FALSE, results='asis'}
start = which(volBars$VPIN > 0)[1]
xtable(subset(volBars[start:(start+5),], select=-c(TimeStart,VolBar)))
```

\newpage
To show the difference in the distributions between the identified states we show a boxplot of the data below Visually we can see that there appears to be a difference in the VPIN values between state 1, and states 3, 4 and 6. We can interpret that as telling us the no-news event appears to be correlated with low values of VPIN, while the private information states tend to be associated with higher VPIN values.

```{r, fig.width = 10, fig.height = 5, fig.fullwidth = FALSE, echo=FALSE, fig.cap="Box plots of VPIN values against decoded HMM states"}
df = data.frame(VolBar=volBars$VolBar, 
                State=factor(volBars$State), 
                VPIN=as.numeric(volBars$VPIN))
melt = melt(df, id.vars = c('VolBar', 'State'))
ggplot(melt, aes(State, value, fill=State)) + geom_boxplot()
```

To further test the strength of this relationship we conduct multiple linear regression of VPIN against State which give us the following results (we define state 1, 3, 4 and 6 as dummy variables with state 1 set as the baseline):
```{r, echo=FALSE, results='asis'}
model.lm = lm(VPIN ~ State, data = df)
xtable(model.lm, caption = "Results of multiple linear regression of VPIN ~ State")
rsq <- summary(model.lm)$r.squared
```

The results of the regression indicate to us that there does indeed appear to be a very strong (p << 0.01) indication that VPIN values associated with the no-news state are going to be significantly lower than VPIN values from the private information states.