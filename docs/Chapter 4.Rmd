\chapter{A Simulation Analysis}

```{r echo=FALSE}
library(ggplot2)
library(reshape2)
library(xtable)
options(xtable.comment = FALSE)
options(xtable.booktabs = TRUE)
```

#Simulate data according to model

We employ a Monte Carlo algorithm to simulate order flow based off the known parameters: $\alpha$ (the probability of information event), $\mu$ (the arrival rate of informed traders) and $\epsilon$ (the arrival rate of uninformed traders). Once we have the simulated order flow we can use the VPIN estimation procedure to give an estimation of the VPIN metric. 


\begin{enumerate}
  \item Set Monte Carlo Parameters:
  \begin{enumerate}
    \item Number of Simulations: $S$
    \item VPIN: ($V$, $n$)
    \item PIN: ($\alpha$, $\mu$)
  \end{enumerate}
  \item Set $\epsilon = \frac{V - \alpha \mu}{2}$, $s = 0$, $j = 0$
  \item $j = j + 1$ 
  \item Draw three random numbers from a $U(0,1)$ distribution: $u_1$, $u_2$, $u_3$ \marginnote{In our R implementation of the VPIN simulation we do not in fact employ $u_3$ as the parameter to the inverse CDF of the Poisson distribution as R already comes with an efficient way to simulate random Poisson variables, namely the function `rpois` }
  \item If $u_1 < \alpha$
  \begin{enumerate}
    \item If $u_2 < \delta$
      \begin{enumerate}
        \item $V_j^B = F^{-1}(u_3, \epsilon)$
        \item $V_j^S = F^{-1}(u_3, \mu + \epsilon)$
      \end{enumerate}
    \item If $u_2 \geq \delta$
      \begin{enumerate}
        \item $V_j^B = F^{-1}(u_3, \mu + \epsilon)$
        \item $V_j^S = F^{-1}(u_3, \epsilon)$
      \end{enumerate}
  \end{enumerate}
  \item If $u_1 < \alpha$
    \begin{enumerate}
      \item $V_j^B = F^{-1}(u_3, \epsilon)$
      \item $V_j^S = V_j^B$
    \end{enumerate}
  \item If $j = n$
    \begin{enumerate}
      \item $j = 0$
      \item $s = s + 1$
      \item $VPIN_s = \frac{\sum_{j=1}^n |V_j^S - V_j^B|}{\sum_{j=1}^n (V_j^S - V_j^B )}$
    \end{enumerate}
  \item If $s < S$, loop to Step 3
  \item Compute results:
    \begin{enumerate}
      \item $E[VPIN] = \frac{1}{S} \sum_{s=1}^S VPIN_s$
      \item $V[VPIN] = \frac{1}{S-1} \sum_{s=1}^S VPIN_s^2 - \frac{1}{S(S-1)} (\sum_{s=1}^S VPIN_s)^2$
    \end{enumerate}
\end{enumerate}

```{r echo=FALSE}
source('../code/VPIN-Sim.R')
```

To illustrate the analysis of the simulated data, we shall examine the steps for one hypothetical trading day, using the following parameters:
```{r, echo=TRUE}
V = 55 #size of each volume bucket
n = 100 #number of volume buckets
alpha = 0.28 #probability of information event
delta = 0.33 #probability of bad news event
mu = 45 #arrival rate of informed trader
epsilon = (V - alpha * mu)/2 #arrival rate of uninformed trader

trades = generate.trades.sim(n, alpha, delta, epsilon, mu)
```

The parameters are therefore as follows:

- $\alpha = `r alpha`$
- $\delta = `r delta`$
- $\mu = `r mu`$
- $\epsilon = `r epsilon`$

#Exploratory analysis of data

By first running some elementary visualisations against the simulated data we can gain an intuitive insight into the patterns that the data exhibit. 

A sinple time series plot of buys and sells appears to show longer periods of relative inactivity which are then interspersed by periods of higher trading activity. This leads us to believe that the data are generated by a mixture of distributions which are regulated by some sort of mixing parameter. As we know in this situation the distributions are indeed driven by $\alpha$ and $\delta$.

```{r, fig.width = 10, fig.height = 4, fig.fullwidth = TRUE, echo=FALSE}
df_melt = melt(trades, id.vars = 'Buckets')
ggplot(df_melt, aes(x = Buckets, y = value)) + 
    geom_line() + 
    facet_wrap(~ variable, scales = 'free_y', ncol = 1)
```

To get a sense of the relative magnitudes of these component distributions, we fit kernel density estimates against the data. Firstly we can see that both of the series are clearly overdispersed compared to a single Poisson distribution. Secondly we notice that both buys and sells appear to have two or three modes each. Again this conforms precisely with our knowledge of the underlying data generating process.

```{r, fig.width = 10, fig.height = 4, fig.fullwidth = TRUE, echo=FALSE}
ggplot(df_melt,aes(x=value, fill=variable)) + geom_density(alpha=0.25)
```


```{r cache=FALSE, echo=FALSE}
source('../code/jump.R')
```

Following the Jump methodology of @Sugar2003a we run the k-means clustering algorithm against the trade imbalance data for $K = 1 \dots 10$. @Sugar2003a provide a rigourous theoretical justification for the ability of the Jump method to calculate the optimum number of clusters. The following output shows that there is an elbow in the distortion levels when the cluster size is three. Again this matches with our knowledge of the underlying data generating process whereby we are either in a) a no-news state b) a news state with private information, or c) a news state without private information. 

```{r, fig.width = 10, fig.height = 7, fig.fullwidth = TRUE, echo=FALSE, results='hide', warning=FALSE}
tradesMatrix <- as.matrix(cbind(trades$Buy, trades$Sell),ncol=2)
colnames(tradesMatrix) <- c("buy", "sell")
temp <- jump(tradesMatrix,y=c(1.5,2,2.5),rand=10,trace=F,plotjumps=TRUE)
```

\newpage
With the optimal number of clusters identified we can now proceed with the k-means analysis on the data and thereby visualise the clusters:

```{r results='hide'}
cl <- kmeans(tradesMatrix, 3)
```


```{r, fig.width = 10, fig.height = 4, fig.fullwidth = TRUE, echo=FALSE}
trades$cluster=factor(cl$cluster)
centers=as.data.frame(cl$centers)
ggplot(data=trades, aes(x=Buy, y=Sell, color=cluster )) + 
 geom_point() + 
 geom_point(data=centers, aes(x=buy,y=sell, color="Center")) +
 geom_point(data=centers, aes(x=buy,y=sell, color="Center"), size=52, alpha=.3, show_guide=FALSE)
```

```{r, echo=FALSE, results='asis'}
clustersDf = data.frame(cbind(cl$centers, cl$size))
names(clustersDf) = c("Buy", "Sell", "Cluster Size")
xtable(clustersDf)
```

Using the theory on trading motivations outlined in section TODO, we can now interpret the cluster centroids in order to find appropriate starting values of $\epsilon$, $\mu$, $\lambda_{b;i}$ and $\lambda_{s;j}$. It is somewhat unrealistically simple given our small simlated data set, but the intuition is rather obvious. We know a priori that there are three states:

\begin{enumerate}
  \item No news event
  \item Good news event
  \item Bad news event
\end{enumerate}

For each state $i$ we know there is a particular $\lambda_{b;i}$ and $\lambda_{s;i}$ 

$$\lambda_b = \begin{pmatrix}
\epsilon_b \\
\epsilon_b + \mu_b \\
\epsilon_b
\end{pmatrix} \qquad
\lambda_s = \begin{pmatrix}
\epsilon_s \\
\epsilon_s \\
\epsilon_s + \mu_s 
\end{pmatrix}$$

We infer that because most trading days (or buckets) do not have private information, we can deduce that the cluster with the greatest size is the one without any private information. From which we can further deduce that the true value $\epsilon$ the arrival rate of the uninformed trader must be close to that cluster's centroid. Based off this justification we identify the following $\epsilon$ values :

```{r echo=FALSE}
attach(clustersDf)
ordered = clustersDf[order(-`Cluster Size`),]
epsilonBuy = ordered$Buy[1]
epsilonSell = ordered$Sell[1]
```

$$\epsilon_b = `r epsilonBuy` \qquad \epsilon_s = `r epsilonSell`$$

Next we infer that the cluster with the largest Buy centroid indicating a large order imbalance is the one with private information after a good news event. Hence:

```{r echo=FALSE}
ordered = clustersDf[order(-Buy),]
lambdaBuy2 = ordered$Buy[1]
muBuy = lambdaBuy2 - epsilonBuy
```

\begin{align}
  \begin{aligned}\nonumber
  \lambda_{b;2} = \epsilon_b + \mu_b \qquad \Rightarrow \qquad  \mu_b &= \lambda_{b;2} - \epsilon_b\\
  &= `r lambdaBuy2` - `r epsilonBuy`\\
  &= `r muBuy`
  \end{aligned}
\end{align}

Similarly we infer that the cluster with the largest Sell centroid indicating a large order imbalance is the one with private information after a bad news event. Hence:

```{r echo=FALSE}
ordered = clustersDf[order(-Sell),]
lambdaSell2 = ordered$Sell[1]
muSell = lambdaSell2 - epsilonSell
```

\begin{align}
  \begin{aligned}\nonumber
  \lambda_{s;2} = \epsilon_s + \mu_s \qquad \Rightarrow \qquad  \mu_s &= \lambda_{s;2} - \epsilon_s\\
  &= `r lambdaSell2` - `r epsilonSell`\\
  &= `r muSell`
  \end{aligned}
\end{align}

Hence we can formulate the initial state parameter estimates:

$$\lambda_b = \begin{pmatrix}
\epsilon_b \\
\epsilon_b + \mu_b \\
\epsilon_b
\end{pmatrix} 
= \begin{pmatrix}
`r epsilonBuy` \\
`r lambdaBuy2` \\
`r epsilonBuy`
\end{pmatrix}\qquad
\lambda_s = \begin{pmatrix}
\epsilon_s \\
\epsilon_s \\
\epsilon_s + \mu_s 
\end{pmatrix}
= \begin{pmatrix}
`r epsilonSell` \\
`r epsilonSell` \\
`r lambdaSell2`
\end{pmatrix}$$


```{r echo=FALSE}
detach(clustersDf)
```

\newpage

#Fit HMM to data given inital values

```{r cache=FALSE, echo=FALSE}
source('../code/A3.R')
```
Run for multiple values of $m$ and outline model selection process via AIC and BIC.

TODO: m=3 should be giving us lowest AIC and BIC, probably still a bug in the code.

```{r cache=FALSE, echo=FALSE, results='hide'}

runEM <- function (lambda_buy, lambda_sell, trades) {
  m_buy = length(lambda_buy)
  m_sell = length(lambda_sell)
  mn <- m_buy * m_sell
  
  delta_buy = matrix(rep(1, mn), ncol = mn)
  delta_buy = delta_buy/apply(delta_buy, 1, sum)
  
  gamma <- matrix(rep(1,mn), nrow = mn, ncol = mn, byrow = TRUE)
  gamma = gamma/apply(gamma,1,sum)
  
  model <- bi.pois.HMM.EM(cbind(trades$Buy,trades$Sell),m_buy,m_sell,lambda_buy,lambda_sell,gamma,delta_buy)
}

lambda_buy = c(epsilonBuy, epsilonBuy + muBuy, epsilonBuy)
lambda_sell = c(epsilonSell,epsilonSell, epsilonSell + muSell)
model = runEM(lambda_buy, lambda_sell, trades)

m1 = runEM(mean(trades$Buy), mean(trades$Sell), trades)
m2 = runEM(c(20,40), c(20,40), trades)
m3 = model
```
```{r}
# m4 = runEM(c(10,20,30,40), c(10,20,30,40), trades)
```

```{r, echo=FALSE, cache=FALSE, results='hide', fig.width = 10, fig.height = 6, fig.fullwidth = FALSE, fig.cap = "Diagostics plot for numbers of states"}

diagDf = data.frame(
  m=c(1,2,3), 
  negLLK = c(m1$mllk, m2$mllk, m3$mllk), 
  BIC = c(m1$BIC, m2$BIC, m3$BIC), 
  AIC = c(m1$AIC, m2$AIC, m3$AIC))

df_melt = melt(diagDf, id.vars = 'm')
ggplot(data=df_melt, aes(x = m, y = value, colour=variable)) + 
    geom_line()
```

Now we present the fitted values for our chosen model, that of $m=3$:

```{r, echo=FALSE, results='asis'}
xtable(model$gamma, caption = "Gamma: Fitted transition probability matrix")

lambdaDf <- data.frame(
  `Lambda Buy`=model$lambda_buy, 
  `Lambda Sell`=model$lambda_sell
  )

xtable(lambdaDf, caption = "Lambda Buy and Sell: Fitted arrival rates of informed and uninformed traders")
```

#Local Decoding to accuracy of hidden state identification

```{r eval=FALSE}
states = bi.pois.HMM.local_decoding(testdata, mn, model$lambda_buy, model$lambda_sell,gamma)
stateProbs = bi.pois.HMM.state_probs(testdata, mn, model$lambda_buy, model$lambda_sell,gamma)
```

On identification of the 'best' model, use this fitted model to run analysis of simulated data to see if expected hidden state matches the predetermined hidden state for all $t = 1 \dots T$ 

Run correlation analysis gainst private information state buckets and VPIN values

#Prediction of subsequent states

Run prediction against cross-validation set and potentially hint at Market Maker implications