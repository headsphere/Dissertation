\chapter{A Simulation Analysis}

```{r echo=FALSE}
library(ggplot2)
library(reshape2)
library(xtable)
options(xtable.comment = FALSE)
options(xtable.booktabs = TRUE)
```

#Simulate data according to model

We employ a Monte Carlo algorithm to simulate order flow based off the known parameters: $\alpha$ (the probability of information event), $\mu$ (the arrival rate of informed traders) and $\epsilon$ (the arrival rate of uninformed traders). Once we have the simulated order flow we can use the VPIN estimation procedure to give an estimation of the VPIN metric. 


\begin{enumerate}
  \item Set Monte Carlo Parameters:
  \begin{enumerate}
    \item Number of Simulations: $S$
    \item VPIN: ($V$, $n$)
    \item PIN: ($\alpha$, $\mu$)
  \end{enumerate}
  \item Set $\epsilon = \frac{V - \alpha \mu}{2}$, $s = 0$, $j = 0$
  \item $j = j + 1$ 
  \item Draw three random numbers from a $U(0,1)$ distribution: $u_1$, $u_2$, $u_3$ \marginnote{In our R implementation of the VPIN simulation we do not in fact employ $u_3$ as the parameter to the inverse CDF of the Poisson distribution as R already comes with an efficient way to simulate random Poisson variables, namely the function `rpois` }
  \item If $u_1 < \alpha$
  \begin{enumerate}
    \item If $u_2 < \delta$
      \begin{enumerate}
        \item $V_j^B = F^{-1}(u_3, \epsilon)$
        \item $V_j^S = F^{-1}(u_3, \mu + \epsilon)$
      \end{enumerate}
    \item If $u_2 \geq \delta$
      \begin{enumerate}
        \item $V_j^B = F^{-1}(u_3, \mu + \epsilon)$
        \item $V_j^S = F^{-1}(u_3, \epsilon)$
      \end{enumerate}
  \end{enumerate}
  \item If $u_1 < \alpha$
    \begin{enumerate}
      \item $V_j^B = F^{-1}(u_3, \epsilon)$
      \item $V_j^S = V_j^B$
    \end{enumerate}
  \item If $j = n$
    \begin{enumerate}
      \item $j = 0$
      \item $s = s + 1$
      \item $VPIN_s = \frac{\sum_{j=1}^n |V_j^S - V_j^B|}{\sum_{j=1}^n (V_j^S - V_j^B )}$
    \end{enumerate}
  \item If $s < S$, loop to Step 3
  \item Compute results:
    \begin{enumerate}
      \item $E[VPIN] = \frac{1}{S} \sum_{s=1}^S VPIN_s$
      \item $V[VPIN] = \frac{1}{S-1} \sum_{s=1}^S VPIN_s^2 - \frac{1}{S(S-1)} (\sum_{s=1}^S VPIN_s)^2$
    \end{enumerate}
\end{enumerate}

```{r echo=FALSE}
source('../code/VPIN-Sim.R')
V = 60 #size of each volume bucket
n = 100 #number of volume buckets
alpha = 0.28 #probability of information event
delta = 0.33 #probability of bad news event
mu = 55 #arrival rate of informed trader
epsilon = (V - alpha * mu)/2 #arrival rate of uninformed trader

trades = generate.trades.sim(n, alpha, delta, epsilon, mu)
trueStates = trades[,c("States"),drop=FALSE]
trades = trades[,c("Buckets","Buy", "Sell"),drop=TRUE]
```

To illustrate the analysis of the simulated data, we shall examine the steps for one hypothetical trading day, using the following parameters:

- $\alpha = `r alpha`$
- $\delta = `r delta`$
- $\mu = `r mu`$
- $\epsilon = `r epsilon`$

#Exploratory analysis of data


By first running some elementary visualisations against the simulated data we can gain an intuitive insight into the patterns that the data exhibit. 

```{r, fig.width = 10, fig.height = 4, fig.fullwidth = TRUE, echo=FALSE, fig.pos='H'}
df_melt = melt(trades, id.vars = 'Buckets')
ggplot(df_melt, aes(x = Buckets, y = value)) + 
    geom_line() + 
    facet_wrap(~ variable, scales = 'free_y', ncol = 1)
```
A simple time series plot of buys and sells appears to show longer periods of relative inactivity which are then interspersed by periods of higher trading activity. This leads us to believe that the data are generated by a mixture of distributions which are regulated by some sort of mixing parameter. As we know in this situation the distributions are indeed driven by $\alpha$ and $\delta$.

To get a sense of the relative magnitudes of these component distributions, we fit kernel density estimates against the data. Firstly we can see that both of the series are clearly overdispersed compared to a single Poisson distribution. Secondly we notice that both buys and sells appear to have two or three modes each. Again this conforms precisely with our knowledge of the underlying data generating process.

```{r, fig.width = 10, fig.height = 4, fig.fullwidth = TRUE, echo=FALSE}
ggplot(df_melt,aes(x=value, fill=variable)) + geom_density(alpha=0.25)
```

\newpage
Following the Jump methodology of @Sugar2003a we run the k-means clustering algorithm against the trade imbalance data for $K = 1 \dots 10$. @Sugar2003a provide a rigourous theoretical justification for the ability of the Jump method to calculate the optimum number of clusters. The following output shows that there is an elbow in the distortion levels when the cluster size is three. Again this matches with our knowledge of the underlying data generating process whereby we are either in a) a no-news state b) a news state with private information, or c) a news state without private information. 

```{r, fig.width = 10, fig.height = 7, fig.fullwidth = TRUE, echo=FALSE, results='hide', warning=FALSE}
source('../code/jump.R')
tradesMatrix <- as.matrix(cbind(trades$Buy, trades$Sell),ncol=2)
colnames(tradesMatrix) <- c("buy", "sell")
temp <- jump(tradesMatrix,y=c(1.5,2,2.5),rand=10,trace=F,plotjumps=TRUE)
```

\newpage
With the optimal number of clusters identified we can now proceed with the k-means analysis on the data and thereby visualise the clusters:

```{r echo=FALSE, results='hide'}
cl <- kmeans(tradesMatrix, 3)
```


```{r, fig.width = 10, fig.height = 4, fig.fullwidth = TRUE, echo=FALSE}
trades$cluster=factor(cl$cluster)
centers=as.data.frame(cl$centers)
ggplot(data=trades, aes(x=Buy, y=Sell, color=cluster )) + 
 geom_point() + 
 geom_point(data=centers, aes(x=buy,y=sell, color="Center")) +
 geom_point(data=centers, aes(x=buy,y=sell, color="Center"), size=52, alpha=.3, show_guide=FALSE)
```

```{r, echo=FALSE, results='asis'}
clustersDf = data.frame(cbind(cl$centers, cl$size))
names(clustersDf) = c("Buy", "Sell", "Cluster Size")
xtable(clustersDf)
```

We can now interpret the cluster centroids in order to find appropriate starting values of $\epsilon$, $\mu$, $\lambda_{b;i}$ and $\lambda_{s;j}$. It is unrealistically simple given our small simulated data set, but the intuition is rather obvious. We know a priori that there are three states:

\begin{enumerate}
  \item No news event
  \item Good news event
  \item Bad news event
\end{enumerate}

For each state $i$ we know there is a particular $\lambda_{b;i}$ and $\lambda_{s;i}$ 

$$\lambda_b = \begin{pmatrix}
\epsilon_b \\
\epsilon_b + \mu_b \\
\epsilon_b
\end{pmatrix} \qquad
\lambda_s = \begin{pmatrix}
\epsilon_s \\
\epsilon_s \\
\epsilon_s + \mu_s 
\end{pmatrix}$$

We infer that because most trading days (or buckets) do not have private information, we can deduce that the cluster with the greatest size is the one without any private information. From which we can further deduce that the true value $\epsilon$ the arrival rate of the uninformed trader must be close to that cluster's centroid. Based off this justification we identify the following $\epsilon$ values :

```{r echo=FALSE}
attach(clustersDf)
ordered = clustersDf[order(-`Cluster Size`),]
epsilonBuy = ordered$Buy[1]
epsilonSell = ordered$Sell[1]
```

$$\epsilon_b = `r epsilonBuy` \qquad \epsilon_s = `r epsilonSell`$$

Next we infer that the cluster with the largest Buy centroid indicating a large order imbalance is the one with private information after a good news event. Hence:

```{r echo=FALSE}
ordered = clustersDf[order(-Buy),]
lambdaBuy2 = ordered$Buy[1]
muBuy = lambdaBuy2 - epsilonBuy
```

\begin{align}
  \begin{aligned}\nonumber
  \lambda_{b;2} = \epsilon_b + \mu_b \qquad \Rightarrow \qquad  \mu_b &= \lambda_{b;2} - \epsilon_b\\
  &= `r lambdaBuy2` - `r epsilonBuy`\\
  &= `r muBuy`
  \end{aligned}
\end{align}

Similarly we infer that the cluster with the largest Sell centroid indicating a large order imbalance is the one with private information after a bad news event. Hence:

```{r echo=FALSE}
ordered = clustersDf[order(-Sell),]
lambdaSell2 = ordered$Sell[1]
muSell = lambdaSell2 - epsilonSell
```

\begin{align}
  \begin{aligned}\nonumber
  \lambda_{s;2} = \epsilon_s + \mu_s \qquad \Rightarrow \qquad  \mu_s &= \lambda_{s;2} - \epsilon_s\\
  &= `r lambdaSell2` - `r epsilonSell`\\
  &= `r muSell`
  \end{aligned}
\end{align}

Hence we can formulate the initial state parameter estimates:

$$\lambda_b = \begin{pmatrix}
\epsilon_b \\
\epsilon_b + \mu_b \\
\epsilon_b
\end{pmatrix} 
= \begin{pmatrix}
`r epsilonBuy` \\
`r lambdaBuy2` \\
`r epsilonBuy`
\end{pmatrix}\qquad
\lambda_s = \begin{pmatrix}
\epsilon_s \\
\epsilon_s \\
\epsilon_s + \mu_s 
\end{pmatrix}
= \begin{pmatrix}
`r epsilonSell` \\
`r epsilonSell` \\
`r lambdaSell2`
\end{pmatrix}$$


```{r echo=FALSE}
detach(clustersDf)
```

\newpage

#Fit HMM to data given inital values

A standard approach for model selection in the HMM literature is via the use of information criteria such as Akaike or Bayesian Information Criteria (AIC and BIC respectively)@zucchini[pp. 89-92]. The theory of these criteria state that the model with the lowest number will be the one that most closely fits data, taking into account the theory of parsimony.  

These criteria are stated as:

$$\text{AIC } = -2 \text{ log } L + 2p$$
$$\text{BIC } = -2 \text{ log } L + p \text{ log } T$$

Where $L$ is the likelihood, $p$ is the number of parameters and $T$ is the number of observations.

We generate AIC and BIC values for a variety of different states $m$ and make a model selection based off these calculation.

```{r cache=TRUE, echo=FALSE, results='hide'}
source('../code/hmmBivPois.R')

runEM <- function (lambda_buy, lambda_sell, trades) {
  m_buy = length(lambda_buy)
  m_sell = length(lambda_sell)
  mn <- m_buy * m_sell
  
  delta_buy = matrix(rep(1, mn), ncol = mn)
  delta_buy = delta_buy/apply(delta_buy, 1, sum)
  
  gamma <- matrix(rep(1,mn), nrow = mn, ncol = mn, byrow = TRUE)
  gamma = gamma/apply(gamma,1,sum)
  
  model <- bi.pois.HMM.EM(cbind(trades$Buy,trades$Sell),m_buy,m_sell,lambda_buy,lambda_sell,gamma,delta_buy)
}

lambda_buy = c(epsilonBuy, epsilonBuy + muBuy, epsilonBuy)
lambda_sell = c(epsilonSell,epsilonSell, epsilonSell + muSell)
model = runEM(lambda_buy, lambda_sell, trades)

m1 = runEM(mean(trades$Buy), mean(trades$Sell), trades)
m2 = runEM(c(20,40), c(20,40), trades)
m3 = model

# Getting NaNs with m=4
# m4 = runEM(c(10,20,30,40), c(10,20,30,40), trades)
```

```{r, echo=FALSE, cache=FALSE, results='hide', fig.width = 10, fig.height = 6, fig.fullwidth = FALSE, fig.cap = "Diagostics plot for numbers of states."}

diagDf = data.frame(
  m=c(1,2,3), 
  negLLK = c(m1$mllk, m2$mllk, m3$mllk), 
  BIC = c(m1$BIC, m2$BIC, m3$BIC), 
  AIC = c(m1$AIC, m2$AIC, m3$AIC))

df_melt = melt(diagDf, id.vars = 'm')
ggplot(data=df_melt, aes(x = m, y = value, colour=variable)) + 
    geom_line()
```

In the above plot we show AIC and BIC values (along with negative log-likelihood for interest) for $m=1,2,3$. We can see that the two and three state models appear to be dramatically lower that the one state model. Even though the three state model has a slightly larger AIC and BIC, for the purposes of interpretation we choose $m=3$ and present the fitted model parameters as follows:

```{r, echo=FALSE, results='asis'}
# xtable(model$gamma, caption = "Gamma: Fitted transition probability matrix")

lambdaDf <- data.frame(
  `Lambda Buy Hat`=model$lambda_buy, 
  `Lambda Sell Hat`=model$lambda_sell
  )

xtable(lambdaDf, caption = "Lambda Buy and Sell: Fitted arrival rates of informed and uninformed traders")
```


Which as can be seen below, appear to be very close to the original parameters of the simulated data, thereby demonstrating to us the effectiveness of the HMM in estimating model parameters.^[A more thorough analysis would involve bootstrapping these parameter estimates in order to generate confidence intervals.]

$$\lambda_b = 
\begin{pmatrix}
  \epsilon_b \\
  \epsilon_b + \mu_b \\
  \epsilon_b
\end{pmatrix} = 
\begin{pmatrix}
  `r epsilon` \\
  `r epsilon + mu` \\
  `r epsilon`
\end{pmatrix} \qquad
\lambda_s = 
\begin{pmatrix}
  \epsilon_s \\
  \epsilon_s \\
  \epsilon_s + \mu_s 
\end{pmatrix}= 
\begin{pmatrix}
  `r epsilon` \\
  `r epsilon` \\
  `r epsilon + mu`
\end{pmatrix}$$

#Local Decoding for Hidden State Identification

On identification of the 'best' model, use this fitted model to run analysis of simulated data to see if expected hidden state matches the predetermined hidden state for all $t = 1 \dots T$ 

Because of the mathematical setup of the bivariate Poisson HMM, we are required to perform a translation step between the true states and the calculated states, somewhat analogously to the process we performed as part of the likelihood calculation whereby we translated parameters from natural to working representations. To illustrate let us look at two hypothetical $\lambda$ vectors for $m=3$:

$$\lambda_b = \begin{pmatrix}
\lambda_{b1} \\
\lambda_{b2} \\
\lambda_{b3} 
\end{pmatrix} \qquad
\lambda_s = \begin{pmatrix}
\lambda_{s1} \\
\lambda_{s2} \\
\lambda_{s3} 
\end{pmatrix}$$

When formulating our parameter matrices, to represent the states in one dimension, we generate the Cartesian product of these parameters representing them with a unique indexer:

```{r, echo=FALSE, results='asis'}
combinsDf = expand.grid(c("b1", "b2", "b3"), c("s1", "s2", "s3"))
names(combinsDf) = c("Lambda Buy", "Lambda Sell")
xtable(combinsDf)
```

In order to interprete the decoded states calculated by the HMM our remaining task is to translate the estimated 'working' state index to the 'natural' state index. When generating our simulated data we apply arbitrary state labels to each of the generated obvservations whereby:

- 1: No news state
- 2: Bad news state
- 3: Good news state

With these labels applied, we can output the simulated true state values:

```{r, fig.width = 10, fig.height = 3, fig.fullwidth = TRUE, echo=FALSE, fig.cap="Untranslated decoded states", warning=FALSE}
mn <- length(model$lambda_buy) * length(model$lambda_sell)
states = bi.pois.HMM.local_decoding(cbind(trades$Buy,trades$Sell), mn, model$lambda_buy, model$lambda_sell,model$gamma)

statesDf = data.frame(Buckets=trades$Buckets, `True States`=trueStates, `Decoded States`=states)

df_melt = melt(statesDf, id.vars = 'Buckets')
ggplot(df_melt, aes(x = Buckets, y = value)) + 
    geom_step() +
    facet_wrap(~ variable, scales = 'free_y', ncol = 1)
```

By examining the contrast between the untranslated decoded states and the true states we can easily interpret the following translation scheme:

- Working State 1 $\Rightarrow$ Natural State 1
- Working State 4 $\Rightarrow$ Natural State 3
- Working State 3 $\Rightarrow$ Natural State 2

And after applying this translation to the decoded states, we see that the decoded states match the true states with 100% accuracy:

```{r, fig.width = 10, fig.height = 3, fig.fullwidth = TRUE, echo=FALSE, fig.cap="Translated decoded states"}
#translate states
statesDf$Decoded.States[statesDf$Decoded.States == 3] <- 2
statesDf$Decoded.States[statesDf$Decoded.States == 4] <- 3

df_melt = melt(statesDf, id.vars = 'Buckets')
ggplot(df_melt, aes(x = Buckets, y = value)) + 
    geom_step() +
    facet_wrap(~ variable, scales = 'free_y', ncol = 1)
```

\newpage

#Analyse Relationship Between Hidden States and VPIN

The final step in our analysis is to make inferences about the rleationship between these decoded hidden states and the calculated VPIN metric. The hypothesis we are examining is whether there is a strong relationship between high VPIN values and private information-based trading. We challenge we face however is determining exactly what we are comparing.

First we briefly repeat how VPIN is calculated. Trading activity is typically measured in time bars, that is a summary of the tick-by-tick activity within a particular time period, e.g. seconds, minutes, hours etc. As part of our simulation analysis up to this point we have created time series showing generated buys and sells aggregated by time bars. As explained in the introduction, due to the difficulties of classifying trading activity as either buy or sell in a high-frequency context, trade data is spliced up into volume bars and these bars are then probabilistically allocated as buys or sells by examining the price differentials within those volume bars. Once the probabilistic buy and sell amounts have been calculated per volume bar, the trading imbalances and hence the VPIN can be calculated. The missing piece of the analysis puzzle is hence how to associate the decoded hidden state of a time bar with the VPIN value of the volume bar. The following is our attempt to define that linkage. ^[The procedure is an extension of the algorithm found in Appendix 1.]

\begin{figure*}
  \includegraphics{images/vpin-sim}
  \label{fig:vpin-sim}
\end{figure*}

Table \ref{fig:vpin-sim} demonstrates a simple example of the process by which we translate from time bars to volume bars. The first table shows a simulated realisation of five time bars. Each bar has a buy and sell value and a decoded state associated with that time bar. In our example we assume that the size of each volume bar is $V=10$. We expand each time bar up into the total number of trades per bar e.g. timebar 1 is split into 3 entries and timebar 2 is split into 6 entries. We continue this for all the time bars. Once all the time bars have been expanded out we group these entries up into volume bars all equally sized at $V$.^[The R code used to implement this translation can be found in Appendix 2.]

In the volume bar schematic, the relative sizes of the bars are all to scale and indicate that as time progresses (left to right) and trading activity increases, the number of volume bars required to represent the time bars increases. The amount of information in each bar remains constant and hence they are directly comparable with each other.

Each time bar that composes the volume bars has an associated time bar state. What we need to do is decide what is the most likely hidden state for the volume bar. To do this we determine the mode of all the states in the volume bar. For example volume bar 1, state 2 is the most frequent state therefore 2 becomes the volume bar state.

The next step is to calculate the probabilistic buy and sell amounts for each volume bar. Because the input to this translation process is in fact signed order flow data, we don't need to infer the trade directions based off the price differentials as in the original VPIN algorithm. Instead we simply take a weighted average of the constituent time bars' buy and sell values. For example because volume bar 1 is composed of 30% from time bar 1, 60% from time bar 2, and 10% from time bar 3, we evaluate volume bar 1's buy and sell amounts as follows:

\begin{align}
  \begin{aligned}\nonumber
  V_{b1} &= 0.3 \times T_{b1} + 0.6 \times T_{b2} + 0.1 \times T_{b3}\\
         &= 0.3 \times 1 + 0.6 \times 2 + 0.1 \times 2\\
         &= 1.7
  \end{aligned}
\end{align}

\begin{align}
  \begin{aligned}\nonumber
  V_{s1} &= 0.3 \times T_{s1} + 0.6 \times T_{s2} + 0.1 \times T_{s3}\\
         &= 0.3 \times 2 + 0.6 \times 4 + 0.1 \times 5\\
         &= 3.5
  \end{aligned}
\end{align}

We then take the absolute difference between these two buy and sell values and divide them by $V$ to get that volume bar's VPIN value:

$$VPIN_{\tau} = \frac{| V_{b\tau} - V_{s\tau }|}{V}$$

\newpage

A small sample of the generated volume bars from the simulated data is shown below: 

```{r, echo=FALSE, results='hide', warning=FALSE, cache=TRUE}
source('../code/VPIN-Timebars.R')
trades = generate.trades.sim(n, alpha, delta, mu, epsilon) 
trades = data.frame(trades,TimeBar=rep(as.POSIXct(NA,"",tz = "GMT"),n))
volBars =  GenerateVolBars(trades, n)
```

```{r, echo=FALSE, results='asis'}
start = which(volBars$VPIN > 0)[1]
xtable(subset(volBars[start:(start+5),], select=-c(TimeStart,VolBar)))
```

A simple visual exploration of the difference in means between the identified states is shown below. Visually we can see that there is a large difference in the VPIN values between state 1, and states 2 and 3. We can interprete that as telling us the no-news event appears to be correlated with low values of VPIN, while the private information states tend to be associated with higher VPIN values.

```{r, fig.width = 10, fig.height = 4, fig.fullwidth = FALSE, echo=FALSE, fig.cap="Box plots of VPIN values against decoded HMM states"}
df = data.frame(VolBar=volBars$VolBar, 
                State=factor(volBars$State), 
                VPIN=as.numeric(volBars$VPIN))
melt = melt(df, id.vars = c('VolBar', 'State'))
ggplot(melt, aes(State, value, fill=State)) + geom_boxplot()
```

To further test the strength of this relationship we conduct multiple linear regression of VPIN against State which give us the following results (we define state 1, 2 and 3 as dummy variables with state 1 set as the baseline):
```{r, echo=FALSE, results='asis'}
model.lm = lm(VPIN ~ State, data = df)
xtable(model.lm)
rsq <- summary(model.lm)$r.squared
```

The results of the regression indicate to us that there does indeed appear to be a very strong (p << 0.01) indication that VPIN values associated with the no-news state are going to be significantly lower than VPIN values from the private information states.